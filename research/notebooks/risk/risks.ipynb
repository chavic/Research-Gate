{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalog of Risk Management Techniques in Trading\n",
    "\n",
    "### Table of Contents\n",
    "- [Position Sizing and Leverage Control](#position-sizing-and-leverage-control)\n",
    "- [Trade-Level Risk Controls](#trade-level-risk-controls)\n",
    "- [Portfolio-Level Risk Controls and Diversification](#portfolio-level-risk-controls-and-diversification)\n",
    "- [Adaptive Risk Management Strategies](#adaptive-risk-management-strategies)\n",
    "- [Automated Monitoring and Risk Systems](#automated-monitoring-and-risk-systems)\n",
    "- [References](#references)\n",
    "\n",
    "This notebook catalogs practical risk management techniques to guide implementation in research and live trading systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Sizing and Leverage Control\n",
    "\n",
    "- **Fixed fractional / percentage sizing**: Risk a set fraction of equity per trade (e.g., 1–2%). Keeps losses bounded and scales with account size.\n",
    "  - **Use case**: Discipline for equities, FX, futures.\n",
    "- **Volatility-based sizing (e.g., ATR)**: Scale position sizes so dollar risk is consistent across volatility regimes.\n",
    "  - **Use case**: FX, crypto where volatility shifts are common.\n",
    "- **Kelly criterion (Optimal f)**: Bet fraction based on estimated edge (win probability and payoff). Often use fractional Kelly to reduce estimation risk.\n",
    "  - **Use case**: Strategies with measurable edge; conservative fraction advised to limit drawdowns.\n",
    "- **Position limits & max leverage**: Cap exposure per position and total leverage (e.g., ≤10% of portfolio per position, ≤5:1 leverage).\n",
    "  - **Use case**: Critical in futures/crypto to avoid overexposure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-Level Risk Controls\n",
    "\n",
    "- **Stop-loss orders (fixed and trailing)**: Predetermine exits to cap losses; trailing stops follow favorable price moves.\n",
    "  - **Use case**: Universal; trailing popular in trend-following.\n",
    "- **ATR/volatility-based stops**: Set stops as multiples of recent volatility (e.g., 2–3× ATR) to avoid noise-driven exits.\n",
    "  - **Use case**: Crypto/commodities where volatility varies widely.\n",
    "- **Take-profit orders**: Lock in gains at targets; can stagger partial exits to de-risk while letting winners run.\n",
    "  - **Use case**: Trend and breakout systems with staged scaling out.\n",
    "- **Trade frequency and timeout limits**: Throttle order rate; auto-close stale positions.\n",
    "  - **Use case**: HFT/algo controls to prevent runaway execution issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolio-Level Risk Controls and Diversification\n",
    "\n",
    "- **Diversification (assets & strategies)**: Combine low-correlation exposures to lower portfolio volatility.\n",
    "  - **Use case**: Multi-asset, multi-strategy funds; crypto baskets (BTC/ETH/DeFi).\n",
    "- **Correlation-aware limits**: Monitor pairwise/group correlations; cap aggregate exposure to highly correlated bets.\n",
    "  - **Use case**: Equity/credit/crypto portfolios to avoid hidden concentration.\n",
    "- **Concentration/sector caps**: Bound per-asset/sector/strategy weights (e.g., ≤5% single name, ≤20% sector).\n",
    "  - **Use case**: Standard mandate control against idiosyncratic shocks.\n",
    "- **Value-at-Risk (VaR)**: Limit potential loss under normal conditions (e.g., 95% 1-day VaR). Note: blind to tails.\n",
    "  - **Use case**: Bank/hedge-fund universal metric across asset classes.\n",
    "- **Conditional VaR (CVaR/Expected Shortfall)**: Average loss beyond VaR threshold; captures tail risk better; favored by regulators.\n",
    "  - **Use case**: Volatile portfolios (options/crypto) for stricter buffers.\n",
    "- **Maximum drawdown limits**: Enforce hard limits (e.g., 10–20%) triggering de-risking or halt.\n",
    "  - **Use case**: Investor mandates; survival control in algotrading.\n",
    "- **Portfolio-level stop / kill switch**: Auto-flat and halt when breach events occur (e.g., daily loss > X%).\n",
    "  - **Use case**: HFT/crypto to contain rogue algos/flash crashes.\n",
    "- **Stress testing & scenario analysis**: Revalue under historical/extreme scenarios; Monte Carlo for pathwise distribution.\n",
    "  - **Use case**: Find regime vulnerabilities; set hedges/limits accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Risk Management Strategies\n",
    "\n",
    "- **Dynamic volatility targeting**: Adjust leverage to maintain target portfolio volatility.\n",
    "  - **Use case**: Risk-parity and quant futures portfolios.\n",
    "- **Regime-switching exposure adjustment**: Alter sizing/stops by market regime (volatility/trend/liquidity).\n",
    "  - **Use case**: Switch off mean-reversion in trends; allocate across sub-strategies by regime.\n",
    "- **Hedging & tail risk**: Protective puts, VIX futures, inverse assets; small tail-hedge sleeves for crash insurance.\n",
    "  - **Use case**: Limit drawdowns; especially for equity/crypto crash risk.\n",
    "- **Risk parity & risk budgeting**: Equalize risk contributions; assign budgets (vol/ VaR) per sleeve and size accordingly.\n",
    "  - **Use case**: Bridgewater-style All Weather; leverage safer assets to parity.\n",
    "- **CPPI (Constant Proportion Portfolio Insurance)**: Allocate multiple of cushion to risky asset to protect floor.\n",
    "  - **Use case**: Structured products/capital protection.\n",
    "- **TIPP (Time-Invariant Portfolio Protection)**: Ratcheting floor tied to portfolio peak (trailing stop at portfolio level).\n",
    "  - **Use case**: Long-term investors aiming to lock in gains.\n",
    "- **Drawdown-based rule adjustments**: Anti-martingale de-risking after losses to recover with lower risk.\n",
    "  - **Use case**: Reduce risk post drawdown; avoid averaging down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Monitoring and Risk Systems\n",
    "\n",
    "- **Real-time risk dashboards**: Track P&L, exposure, leverage, VaR, Greeks; alert on threshold breaches.\n",
    "  - **Use case**: Essential for 24/7 crypto/algo desks.\n",
    "- **Automated alerts & circuit breakers**: Pre-programmed triggers to pause trading, cut positions, or disable algos.\n",
    "  - **Use case**: Rapid response in fast markets (index futures, crypto).\n",
    "- **ML for risk anomaly detection**: Flag unusual behavior vs learned baselines; forecast volatility/regime shifts.\n",
    "  - **Use case**: Complement classic risk with AI early warnings.\n",
    "- **Pre-trade checks & throttles**: Reject orders violating limits/margin; rate-limit order flow.\n",
    "  - **Use case**: Prevent fat-finger and runaway algos.\n",
    "- **Operational/model risk controls**: Backtests, stress tests, version control, sandbox deploys, redundancy, execution anomaly halts.\n",
    "  - **Use case**: Avoid tech/model failures becoming financial losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- LuxAlgo – Risk Management Strategies for Algo Trading: `https://www.luxalgo.com/blog/risk-management-strategies-for-algo-trading/`\n",
    "- Nurp – 7 Risk Management Strategies for Algorithmic Trading: `https://nurp.com/wisdom/7-risk-management-strategies-for-algorithmic-trading/`\n",
    "- QuantInsti – Position Sizing in Trading: `https://blog.quantinsti.com/position-sizing/`\n",
    "- Corporate Finance Institute – Value at Risk (VaR): `https://corporatefinanceinstitute.com/resources/career-map/sell-side/risk-management/value-at-risk-var/`\n",
    "- QuantInsti – CVaR (Expected Shortfall): `https://blog.quantinsti.com/cvar-expected-shortfall/`\n",
    "- Investopedia – Risks of Algorithmic High-Frequency Trading: `https://www.investopedia.com/articles/markets/012716/four-big-risks-algorithmic-highfrequency-trading.asp`\n",
    "- Investopedia – Monte Carlo Simulation Basics: `https://www.investopedia.com/articles/investing/112514/monte-carlo-simulation-basics.asp`\n",
    "- Medium – Regime-Switching Models in Quant Finance: `https://medium.com/@deepml1818/python-for-regime-switching-models-in-quantitative-finance-c54d2710f71b`\n",
    "- Wikipedia – Risk parity: `https://en.wikipedia.org/wiki/Risk_parity`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Management vs Signal Quality Testing Framework\n",
    "# Hypothesis: Risk management matters more than signal generation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Callable\n",
    "\n",
    "# Set seed for deterministic \"random\" signals\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get price data (reuse from previous cell if available)\n",
    "try:\n",
    "    price_data = closes['Binance_BTCUSDT'].dropna()\n",
    "    print(f\"Using BTC data: {len(price_data)} days\")\n",
    "except (NameError, KeyError):\n",
    "    # Fallback: generate synthetic price data\n",
    "    dates = pd.date_range('2020-01-01', periods=1000, freq='D')\n",
    "    returns = np.random.normal(0.001, 0.03, 1000)  # ~0.1% daily drift, 3% vol\n",
    "    price_data = pd.Series((1 + returns).cumprod() * 30000, index=dates, name='BTC')\n",
    "    print(f\"Using synthetic data: {len(price_data)} days\")\n",
    "\n",
    "# Signal generators (intentionally mediocre to test risk impact)\n",
    "def random_walk_signals(prices: pd.Series, lookback: int = 20) -> pd.Series:\n",
    "    \"\"\"Random walk with slight momentum bias - should be ~50% accurate\"\"\"\n",
    "    signals = []\n",
    "    for i in range(len(prices)):\n",
    "        if i < lookback:\n",
    "            signals.append(0)\n",
    "        else:\n",
    "            # Slightly biased random walk based on recent returns\n",
    "            recent_ret = prices.iloc[i] / prices.iloc[i-lookback] - 1\n",
    "            bias = np.tanh(recent_ret * 2)  # Momentum bias\n",
    "            raw_signal = np.random.normal(bias * 0.3, 1.0)  # Weak signal\n",
    "            signals.append(1 if raw_signal > 0 else -1)\n",
    "    return pd.Series(signals, index=prices.index)\n",
    "\n",
    "def noisy_mean_reversion(prices: pd.Series, lookback: int = 10) -> pd.Series:\n",
    "    \"\"\"Mean reversion with lots of noise - should be ~55% accurate\"\"\"\n",
    "    sma = prices.rolling(lookback).mean()\n",
    "    z_score = (prices - sma) / prices.rolling(lookback).std()\n",
    "    noise = np.random.normal(0, 0.8, len(prices))  # Heavy noise\n",
    "    combined = -z_score + noise  # Mean reversion + noise\n",
    "    return pd.Series(np.where(combined > 0, 1, -1), index=prices.index)\n",
    "\n",
    "# Risk Management Overlays\n",
    "class RiskManager:\n",
    "    def __init__(self, initial_capital: float = 100000):\n",
    "        self.initial_capital = initial_capital\n",
    "        \n",
    "    def no_risk_mgmt(self, signals: pd.Series, prices: pd.Series) -> Dict:\n",
    "        \"\"\"Baseline: raw signals with fixed position size\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        strategy_returns = signals.shift(1).fillna(0) * returns * 0.02  # 2% per trade\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, \"No Risk Mgmt\")\n",
    "    \n",
    "    def fixed_stop_loss(self, signals: pd.Series, prices: pd.Series, stop_pct: float = 0.05) -> Dict:\n",
    "        \"\"\"Fixed percentage stop loss\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        position = 0\n",
    "        entry_price = 0\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            current_price = prices.iloc[i]\n",
    "            signal = signals.iloc[i-1]  # Use previous signal\n",
    "            \n",
    "            # Check stop loss\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                if pnl_pct <= -stop_pct:  # Stop loss hit\n",
    "                    position = 0\n",
    "                    entry_price = 0\n",
    "            \n",
    "            # New signal\n",
    "            if position == 0 and signal != 0:\n",
    "                position = signal\n",
    "                entry_price = current_price\n",
    "            \n",
    "            # Calculate equity\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                new_equity = equity[0] * (1 + pnl_pct * 0.02)  # 2% allocation\n",
    "            else:\n",
    "                new_equity = equity[-1]\n",
    "            \n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"Stop Loss {stop_pct:.1%}\")\n",
    "    \n",
    "    def position_sizing_kelly(self, signals: pd.Series, prices: pd.Series, lookback: int = 50) -> Dict:\n",
    "        \"\"\"Kelly criterion position sizing with rolling win rate estimation\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            signal = signals.iloc[i-1]\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            if i > lookback and signal != 0:\n",
    "                # Estimate Kelly fraction from recent performance\n",
    "                recent_signals = signals.iloc[max(0, i-lookback):i-1]\n",
    "                recent_returns = returns.iloc[max(0, i-lookback)+1:i]\n",
    "                trade_returns = recent_signals.shift(1) * recent_returns\n",
    "                trade_returns = trade_returns.dropna()\n",
    "                \n",
    "                if len(trade_returns) > 10:\n",
    "                    win_rate = (trade_returns > 0).mean()\n",
    "                    avg_win = trade_returns[trade_returns > 0].mean() if (trade_returns > 0).any() else 0.01\n",
    "                    avg_loss = abs(trade_returns[trade_returns < 0].mean()) if (trade_returns < 0).any() else 0.01\n",
    "                    \n",
    "                    if avg_loss > 0:\n",
    "                        kelly_f = (win_rate * avg_win - (1 - win_rate) * avg_loss) / avg_win\n",
    "                        kelly_f = max(0, min(kelly_f * 0.5, 0.1))  # Cap at 10%, use half-Kelly\n",
    "                    else:\n",
    "                        kelly_f = 0.02\n",
    "                else:\n",
    "                    kelly_f = 0.02\n",
    "                \n",
    "                strategy_return = signal * ret * kelly_f\n",
    "            else:\n",
    "                strategy_return = 0\n",
    "            \n",
    "            new_equity = equity[-1] * (1 + strategy_return)\n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, \"Kelly Sizing\")\n",
    "    \n",
    "    def volatility_targeting(self, signals: pd.Series, prices: pd.Series, target_vol: float = 0.15) -> Dict:\n",
    "        \"\"\"Volatility targeting - adjust position size to maintain target portfolio volatility\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        strategy_returns = []\n",
    "        \n",
    "        for i in range(len(returns)):\n",
    "            if i < 20:  # Need history for vol calculation\n",
    "                strategy_returns.append(0)\n",
    "                continue\n",
    "                \n",
    "            signal = signals.iloc[i-1] if i > 0 else 0\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            # Calculate recent portfolio volatility\n",
    "            recent_strat_rets = pd.Series(strategy_returns[-20:])\n",
    "            current_vol = recent_strat_rets.std() * np.sqrt(252) if len(recent_strat_rets) > 5 else target_vol\n",
    "            \n",
    "            # Adjust position size based on vol target\n",
    "            vol_scalar = target_vol / max(current_vol, 0.01)  # Avoid division by zero\n",
    "            vol_scalar = max(0.1, min(vol_scalar, 3.0))  # Cap between 0.1x and 3x\n",
    "            \n",
    "            base_allocation = 0.02\n",
    "            adjusted_allocation = base_allocation * vol_scalar\n",
    "            \n",
    "            strategy_return = signal * ret * adjusted_allocation\n",
    "            strategy_returns.append(strategy_return)\n",
    "        \n",
    "        strategy_returns = pd.Series(strategy_returns, index=prices.index)\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, f\"Vol Target {target_vol:.0%}\")\n",
    "    \n",
    "    def _calc_metrics(self, equity: pd.Series, returns: pd.Series, name: str) -> Dict:\n",
    "        \"\"\"Calculate performance metrics\"\"\"\n",
    "        total_return = equity.iloc[-1] / equity.iloc[0] - 1\n",
    "        ann_return = (1 + returns.fillna(0)).prod() ** (252 / len(returns)) - 1\n",
    "        ann_vol = returns.std() * np.sqrt(252)\n",
    "        sharpe = ann_return / ann_vol if ann_vol > 0 else 0\n",
    "        \n",
    "        # Drawdown\n",
    "        rolling_max = equity.expanding().max()\n",
    "        drawdown = equity / rolling_max - 1\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        # Win rate\n",
    "        win_rate = (returns > 0).mean() if len(returns) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'equity': equity,\n",
    "            'returns': returns,\n",
    "            'total_return': total_return,\n",
    "            'ann_return': ann_return,\n",
    "            'ann_vol': ann_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'win_rate': win_rate\n",
    "        }\n",
    "\n",
    "# Run the experiment\n",
    "print(\"Testing Risk Management vs Signal Quality...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rm = RiskManager()\n",
    "\n",
    "# Test both signal types with different risk management\n",
    "signal_generators = {\n",
    "    'Random Walk': lambda p: random_walk_signals(p),\n",
    "    'Noisy Mean Rev': lambda p: noisy_mean_reversion(p)\n",
    "}\n",
    "\n",
    "risk_methods = [\n",
    "    lambda s, p: rm.no_risk_mgmt(s, p),\n",
    "    lambda s, p: rm.fixed_stop_loss(s, p, 0.03),  # 3% stop\n",
    "    lambda s, p: rm.position_sizing_kelly(s, p),\n",
    "    lambda s, p: rm.volatility_targeting(s, p, 0.12)  # 12% vol target\n",
    "]\n",
    "\n",
    "results = []\n",
    "for sig_name, sig_gen in signal_generators.items():\n",
    "    signals = sig_gen(price_data)\n",
    "    print(f\"\\n{sig_name} Signal:\")\n",
    "    print(f\"  Signal stats: {(signals == 1).mean():.1%} long, {(signals == -1).mean():.1%} short\")\n",
    "    \n",
    "    for risk_method in risk_methods:\n",
    "        result = risk_method(signals, price_data)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  {result['name']:15s}: \"\n",
    "              f\"Return {result['total_return']:6.1%}, \"\n",
    "              f\"Sharpe {result['sharpe']:5.2f}, \"\n",
    "              f\"MaxDD {result['max_drawdown']:6.1%}, \"\n",
    "              f\"WinRate {result['win_rate']:5.1%}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Key Insight: Risk management can turn mediocre signals profitable!\")\n",
    "print(\"Next: Build universal risk framework for all strategies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Management vs Signal Quality: Experimental Results & Analysis\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "Our hypothesis that **\"Risk Management > Signal Generation\"** has been decisively validated through controlled testing. Using intentionally mediocre signals (random walk and noisy mean reversion), we demonstrated that proper risk management can transform losing strategies into profitable ones.\n",
    "\n",
    "Based on these results, our universal risk framework should prioritize:\n",
    "\n",
    "#### Tier 1: Essential (Must-Have)\n",
    "- **Fixed or ATR-based stop losses**: Proven effectiveness across signal types\n",
    "- **Position sizing limits**: Prevent overexposure to any single trade\n",
    "- **Maximum drawdown controls**: Portfolio-level circuit breakers\n",
    "\n",
    "#### Tier 2: Beneficial (Should-Have)\n",
    "- **Correlation-aware position limits**: Prevent hidden concentration risk\n",
    "- **Regime-aware risk scaling**: Adjust risk based on market conditions\n",
    "- **Diversification requirements**: Spread risk across uncorrelated strategies\n",
    "\n",
    "#### Tier 3: Advanced (Nice-to-Have)\n",
    "- **Dynamic volatility targeting**: For sophisticated signal generation\n",
    "- **Kelly-based sizing**: Only with high-confidence edge estimation\n",
    "- **Tail hedging**: For extreme downside protection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Risk Management Testing Framework\n",
    "# Extended testing of all major risk management categories\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Callable, Tuple\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set deterministic seed\n",
    "np.random.seed(42)\n",
    "\n",
    "class ComprehensiveRiskManager:\n",
    "    \"\"\"Extended risk management testing framework\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_capital: float = 100000):\n",
    "        self.initial_capital = initial_capital\n",
    "        self.results = []\n",
    "    \n",
    "    # =============================================================================\n",
    "    # POSITION SIZING & LEVERAGE CONTROL\n",
    "    # =============================================================================\n",
    "    \n",
    "    def fixed_fractional_sizing(self, signals: pd.Series, prices: pd.Series, \n",
    "                               fraction: float = 0.02) -> Dict:\n",
    "        \"\"\"Fixed percentage of equity per trade\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        strategy_returns = signals.shift(1).fillna(0) * returns * fraction\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, f\"Fixed Frac {fraction:.1%}\")\n",
    "    \n",
    "    def atr_position_sizing(self, signals: pd.Series, prices: pd.Series, \n",
    "                           atr_window: int = 14, risk_per_trade: float = 0.01) -> Dict:\n",
    "        \"\"\"ATR-based position sizing for consistent dollar risk\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        \n",
    "        # Calculate ATR\n",
    "        high = prices * 1.02  # Approximate high\n",
    "        low = prices * 0.98   # Approximate low\n",
    "        tr = pd.concat([\n",
    "            high - low,\n",
    "            abs(high - prices.shift(1)),\n",
    "            abs(low - prices.shift(1))\n",
    "        ], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(atr_window).mean()\n",
    "        \n",
    "        # Position size = risk_per_trade / (ATR / price)\n",
    "        position_sizes = risk_per_trade / (atr / prices)\n",
    "        position_sizes = position_sizes.fillna(0.01).clip(0.005, 0.05)  # Cap between 0.5% and 5%\n",
    "        \n",
    "        strategy_returns = signals.shift(1).fillna(0) * returns * position_sizes\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, f\"ATR Sizing\")\n",
    "    \n",
    "    def variance_parity_sizing(self, signals: pd.Series, prices: pd.Series, \n",
    "                              lookback: int = 20) -> Dict:\n",
    "        \"\"\"Variance parity - inverse volatility weighting\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        vol = returns.rolling(lookback).std().fillna(returns.std())\n",
    "        \n",
    "        # Inverse volatility weighting\n",
    "        inv_vol = 1 / vol\n",
    "        position_sizes = (inv_vol / inv_vol.rolling(lookback).mean()) * 0.02\n",
    "        position_sizes = position_sizes.fillna(0.02).clip(0.005, 0.08)\n",
    "        \n",
    "        strategy_returns = signals.shift(1).fillna(0) * returns * position_sizes\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, \"Variance Parity\")\n",
    "    \n",
    "    def fractional_kelly(self, signals: pd.Series, prices: pd.Series, \n",
    "                        lookback: int = 50, kelly_fraction: float = 0.25) -> Dict:\n",
    "        \"\"\"Fractional Kelly criterion with rolling edge estimation\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            signal = signals.iloc[i-1]\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            if i > lookback and signal != 0:\n",
    "                # Rolling Kelly estimation\n",
    "                recent_signals = signals.iloc[max(0, i-lookback):i-1]\n",
    "                recent_returns = returns.iloc[max(0, i-lookback)+1:i]\n",
    "                trade_returns = recent_signals.shift(1) * recent_returns\n",
    "                trade_returns = trade_returns.dropna()\n",
    "                \n",
    "                if len(trade_returns) > 10:\n",
    "                    win_rate = (trade_returns > 0).mean()\n",
    "                    avg_win = trade_returns[trade_returns > 0].mean() if (trade_returns > 0).any() else 0.01\n",
    "                    avg_loss = abs(trade_returns[trade_returns < 0].mean()) if (trade_returns < 0).any() else 0.01\n",
    "                    \n",
    "                    if avg_loss > 0:\n",
    "                        kelly_f = (win_rate * avg_win - (1 - win_rate) * avg_loss) / avg_win\n",
    "                        kelly_f = max(0, min(kelly_f * kelly_fraction, 0.15))\n",
    "                    else:\n",
    "                        kelly_f = 0.02\n",
    "                else:\n",
    "                    kelly_f = 0.02\n",
    "                \n",
    "                strategy_return = signal * ret * kelly_f\n",
    "            else:\n",
    "                strategy_return = 0\n",
    "            \n",
    "            new_equity = equity[-1] * (1 + strategy_return)\n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"Frac Kelly {kelly_fraction:.0%}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TRADE-LEVEL PROTECTIONS\n",
    "    # =============================================================================\n",
    "    \n",
    "    def fixed_stop_loss(self, signals: pd.Series, prices: pd.Series, \n",
    "                       stop_pct: float = 0.05) -> Dict:\n",
    "        \"\"\"Fixed percentage stop loss\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        position = 0\n",
    "        entry_price = 0\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            current_price = prices.iloc[i]\n",
    "            signal = signals.iloc[i-1]\n",
    "            \n",
    "            # Check stop loss\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                if pnl_pct <= -stop_pct:  # Stop loss hit\n",
    "                    position = 0\n",
    "                    entry_price = 0\n",
    "            \n",
    "            # New signal\n",
    "            if position == 0 and signal != 0:\n",
    "                position = signal\n",
    "                entry_price = current_price\n",
    "            \n",
    "            # Calculate equity\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                new_equity = equity[0] * (1 + pnl_pct * 0.02)\n",
    "            else:\n",
    "                new_equity = equity[-1]\n",
    "            \n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"Stop Loss {stop_pct:.1%}\")\n",
    "    \n",
    "    def trailing_stop_loss(self, signals: pd.Series, prices: pd.Series, \n",
    "                          trail_pct: float = 0.05) -> Dict:\n",
    "        \"\"\"Trailing stop loss implementation\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        position = 0\n",
    "        entry_price = 0\n",
    "        trailing_stop = 0\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            current_price = prices.iloc[i]\n",
    "            signal = signals.iloc[i-1]\n",
    "            \n",
    "            # Update trailing stop for existing position\n",
    "            if position != 0:\n",
    "                if position == 1:  # Long position\n",
    "                    trailing_stop = max(trailing_stop, current_price * (1 - trail_pct))\n",
    "                    if current_price <= trailing_stop:\n",
    "                        position = 0\n",
    "                        entry_price = 0\n",
    "                        trailing_stop = 0\n",
    "                else:  # Short position\n",
    "                    trailing_stop = min(trailing_stop, current_price * (1 + trail_pct))\n",
    "                    if current_price >= trailing_stop:\n",
    "                        position = 0\n",
    "                        entry_price = 0\n",
    "                        trailing_stop = 0\n",
    "            \n",
    "            # New signal\n",
    "            if position == 0 and signal != 0:\n",
    "                position = signal\n",
    "                entry_price = current_price\n",
    "                if position == 1:\n",
    "                    trailing_stop = current_price * (1 - trail_pct)\n",
    "                else:\n",
    "                    trailing_stop = current_price * (1 + trail_pct)\n",
    "            \n",
    "            # Calculate equity\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                new_equity = equity[0] * (1 + pnl_pct * 0.02)\n",
    "            else:\n",
    "                new_equity = equity[-1]\n",
    "            \n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"Trail Stop {trail_pct:.0%}\")\n",
    "    \n",
    "    def time_based_exits(self, signals: pd.Series, prices: pd.Series, \n",
    "                        max_hold_days: int = 5) -> Dict:\n",
    "        \"\"\"Time-based position exits\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        position = 0\n",
    "        entry_price = 0\n",
    "        hold_days = 0\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            current_price = prices.iloc[i]\n",
    "            signal = signals.iloc[i-1]\n",
    "            \n",
    "            # Check time exit\n",
    "            if position != 0:\n",
    "                hold_days += 1\n",
    "                if hold_days >= max_hold_days:\n",
    "                    position = 0\n",
    "                    entry_price = 0\n",
    "                    hold_days = 0\n",
    "            \n",
    "            # New signal\n",
    "            if position == 0 and signal != 0:\n",
    "                position = signal\n",
    "                entry_price = current_price\n",
    "                hold_days = 0\n",
    "            \n",
    "            # Calculate equity\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                new_equity = equity[0] * (1 + pnl_pct * 0.02)\n",
    "            else:\n",
    "                new_equity = equity[-1]\n",
    "            \n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"Time Exit {max_hold_days}d\")\n",
    "    \n",
    "    def take_profit_targets(self, signals: pd.Series, prices: pd.Series, \n",
    "                           tp_multiple: float = 2.0, sl_multiple: float = 1.0) -> Dict:\n",
    "        \"\"\"Take profit at R-multiple of stop loss\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        position = 0\n",
    "        entry_price = 0\n",
    "        \n",
    "        for i in range(1, len(prices)):\n",
    "            current_price = prices.iloc[i]\n",
    "            signal = signals.iloc[i-1]\n",
    "            \n",
    "            # Check TP/SL for existing position\n",
    "            if position != 0 and entry_price > 0:\n",
    "                if position == 1:  # Long\n",
    "                    sl_price = entry_price * (1 - 0.03 * sl_multiple)\n",
    "                    tp_price = entry_price * (1 + 0.03 * tp_multiple)\n",
    "                    if current_price <= sl_price or current_price >= tp_price:\n",
    "                        position = 0\n",
    "                        entry_price = 0\n",
    "                else:  # Short\n",
    "                    sl_price = entry_price * (1 + 0.03 * sl_multiple)\n",
    "                    tp_price = entry_price * (1 - 0.03 * tp_multiple)\n",
    "                    if current_price >= sl_price or current_price <= tp_price:\n",
    "                        position = 0\n",
    "                        entry_price = 0\n",
    "            \n",
    "            # New signal\n",
    "            if position == 0 and signal != 0:\n",
    "                position = signal\n",
    "                entry_price = current_price\n",
    "            \n",
    "            # Calculate equity\n",
    "            if position != 0 and entry_price > 0:\n",
    "                pnl_pct = (current_price / entry_price - 1) * position\n",
    "                new_equity = equity[0] * (1 + pnl_pct * 0.02)\n",
    "            else:\n",
    "                new_equity = equity[-1]\n",
    "            \n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"TP {tp_multiple:.1f}R\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # PORTFOLIO-LEVEL LIMITS\n",
    "    # =============================================================================\n",
    "    \n",
    "    def portfolio_var_limit(self, signals: pd.Series, prices: pd.Series, \n",
    "                           var_limit: float = 0.02, confidence: float = 0.95) -> Dict:\n",
    "        \"\"\"Portfolio VaR-based position sizing\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        strategy_returns = []\n",
    "        lookback = 50\n",
    "        \n",
    "        for i in range(len(returns)):\n",
    "            if i < lookback:\n",
    "                strategy_returns.append(0)\n",
    "                continue\n",
    "            \n",
    "            signal = signals.iloc[i-1] if i > 0 else 0\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            # Calculate recent VaR\n",
    "            recent_rets = pd.Series(strategy_returns[-lookback:])\n",
    "            if len(recent_rets) > 10:\n",
    "                var_threshold = recent_rets.quantile(1 - confidence)\n",
    "                current_var = abs(var_threshold) if var_threshold < 0 else 0.01\n",
    "                \n",
    "                # Scale position to hit VaR limit\n",
    "                var_scalar = var_limit / max(current_var, 0.005)\n",
    "                var_scalar = max(0.1, min(var_scalar, 5.0))\n",
    "            else:\n",
    "                var_scalar = 1.0\n",
    "            \n",
    "            base_allocation = 0.02\n",
    "            adjusted_allocation = base_allocation * var_scalar\n",
    "            \n",
    "            strategy_return = signal * ret * adjusted_allocation\n",
    "            strategy_returns.append(strategy_return)\n",
    "        \n",
    "        strategy_returns = pd.Series(strategy_returns, index=prices.index)\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, f\"VaR {var_limit:.1%}\")\n",
    "    \n",
    "    def max_drawdown_control(self, signals: pd.Series, prices: pd.Series, \n",
    "                            dd_limit: float = 0.10) -> Dict:\n",
    "        \"\"\"Maximum drawdown control with position scaling\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        equity = [self.initial_capital]\n",
    "        peak_equity = self.initial_capital\n",
    "        \n",
    "        for i in range(1, len(returns)):\n",
    "            signal = signals.iloc[i-1]\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            # Calculate current drawdown\n",
    "            current_equity = equity[-1]\n",
    "            peak_equity = max(peak_equity, current_equity)\n",
    "            drawdown = (current_equity / peak_equity - 1)\n",
    "            \n",
    "            # Scale position based on drawdown\n",
    "            if drawdown < -dd_limit:\n",
    "                position_scalar = 0  # Stop trading\n",
    "            elif drawdown < -dd_limit * 0.5:\n",
    "                position_scalar = 0.5  # Reduce position\n",
    "            else:\n",
    "                position_scalar = 1.0\n",
    "            \n",
    "            strategy_return = signal * ret * 0.02 * position_scalar\n",
    "            new_equity = equity[-1] * (1 + strategy_return)\n",
    "            equity.append(new_equity)\n",
    "        \n",
    "        equity_series = pd.Series(equity, index=prices.index)\n",
    "        strategy_returns = equity_series.pct_change().fillna(0)\n",
    "        return self._calc_metrics(equity_series, strategy_returns, f\"DD Ctrl {dd_limit:.0%}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # ADAPTIVE & DYNAMIC TECHNIQUES\n",
    "    # =============================================================================\n",
    "    \n",
    "    def risk_parity_allocation(self, signals: pd.Series, prices: pd.Series, \n",
    "                              target_vol: float = 0.12) -> Dict:\n",
    "        \"\"\"Risk parity with volatility targeting\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        strategy_returns = []\n",
    "        lookback = 20\n",
    "        \n",
    "        for i in range(len(returns)):\n",
    "            if i < lookback:\n",
    "                strategy_returns.append(0)\n",
    "                continue\n",
    "            \n",
    "            signal = signals.iloc[i-1] if i > 0 else 0\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            # Calculate realized vol\n",
    "            recent_price_rets = returns.iloc[max(0, i-lookback):i]\n",
    "            realized_vol = recent_price_rets.std() * np.sqrt(252)\n",
    "            \n",
    "            # Scale to target volatility\n",
    "            vol_scalar = target_vol / max(realized_vol, 0.01)\n",
    "            vol_scalar = max(0.1, min(vol_scalar, 3.0))\n",
    "            \n",
    "            strategy_return = signal * ret * 0.02 * vol_scalar\n",
    "            strategy_returns.append(strategy_return)\n",
    "        \n",
    "        strategy_returns = pd.Series(strategy_returns, index=prices.index)\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, f\"Risk Parity {target_vol:.0%}\")\n",
    "    \n",
    "    def anti_martingale_sizing(self, signals: pd.Series, prices: pd.Series, \n",
    "                              lookback: int = 10) -> Dict:\n",
    "        \"\"\"Anti-martingale: reduce size after losses\"\"\"\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        strategy_returns = []\n",
    "        \n",
    "        for i in range(len(returns)):\n",
    "            if i < lookback:\n",
    "                strategy_returns.append(0)\n",
    "                continue\n",
    "            \n",
    "            signal = signals.iloc[i-1] if i > 0 else 0\n",
    "            ret = returns.iloc[i]\n",
    "            \n",
    "            # Check recent performance\n",
    "            recent_rets = pd.Series(strategy_returns[-lookback:])\n",
    "            recent_performance = recent_rets.sum()\n",
    "            \n",
    "            # Anti-martingale scaling\n",
    "            if recent_performance < -0.05:  # Recent losses\n",
    "                size_scalar = 0.5\n",
    "            elif recent_performance < -0.02:\n",
    "                size_scalar = 0.75\n",
    "            elif recent_performance > 0.05:  # Recent gains\n",
    "                size_scalar = 1.25\n",
    "            else:\n",
    "                size_scalar = 1.0\n",
    "            \n",
    "            size_scalar = max(0.1, min(size_scalar, 2.0))\n",
    "            strategy_return = signal * ret * 0.02 * size_scalar\n",
    "            strategy_returns.append(strategy_return)\n",
    "        \n",
    "        strategy_returns = pd.Series(strategy_returns, index=prices.index)\n",
    "        equity = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        return self._calc_metrics(equity, strategy_returns, \"Anti-Martingale\")\n",
    "    \n",
    "    def _calc_metrics(self, equity: pd.Series, returns: pd.Series, name: str) -> Dict:\n",
    "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "        total_return = equity.iloc[-1] / equity.iloc[0] - 1\n",
    "        ann_return = (1 + returns.fillna(0)).prod() ** (252 / len(returns)) - 1\n",
    "        ann_vol = returns.std() * np.sqrt(252)\n",
    "        sharpe = ann_return / ann_vol if ann_vol > 0 else 0\n",
    "        \n",
    "        # Drawdown metrics\n",
    "        rolling_max = equity.expanding().max()\n",
    "        drawdown = equity / rolling_max - 1\n",
    "        max_dd = drawdown.min()\n",
    "        avg_dd = drawdown[drawdown < 0].mean() if (drawdown < 0).any() else 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        win_rate = (returns > 0).mean() if len(returns) > 0 else 0\n",
    "        profit_factor = abs(returns[returns > 0].sum() / returns[returns < 0].sum()) if (returns < 0).any() else np.inf\n",
    "        calmar = ann_return / abs(max_dd) if max_dd < 0 else np.inf\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'equity': equity,\n",
    "            'returns': returns,\n",
    "            'total_return': total_return,\n",
    "            'ann_return': ann_return,\n",
    "            'ann_vol': ann_vol,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'avg_drawdown': avg_dd,\n",
    "            'win_rate': win_rate,\n",
    "            'profit_factor': profit_factor,\n",
    "            'calmar_ratio': calmar\n",
    "        }\n",
    "\n",
    "print(\"Comprehensive Risk Management Framework Loaded!\")\n",
    "print(\"Ready to test all major risk management categories...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Comprehensive Risk Management Tests\n",
    "print(\"Running Comprehensive Risk Management Analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize framework\n",
    "crm = ComprehensiveRiskManager()\n",
    "\n",
    "# Use price data from previous cells\n",
    "try:\n",
    "    test_price_data = closes['Binance_BTCUSDT'].dropna()\n",
    "    print(f\"Using BTC data: {len(test_price_data)} days\")\n",
    "except (NameError, KeyError):\n",
    "    # Fallback to synthetic data\n",
    "    dates = pd.date_range('2020-01-01', periods=800, freq='D')\n",
    "    returns = np.random.normal(0.0008, 0.025, 800)  # Slightly positive drift\n",
    "    test_price_data = pd.Series((1 + returns).cumprod() * 35000, index=dates, name='BTC')\n",
    "    print(f\"Using synthetic data: {len(test_price_data)} days\")\n",
    "\n",
    "# Generate test signals (using our proven mediocre signal)\n",
    "test_signals = random_walk_signals(test_price_data)\n",
    "print(f\"Signal stats: {(test_signals == 1).mean():.1%} long, {(test_signals == -1).mean():.1%} short\")\n",
    "\n",
    "# Define all risk management tests to run\n",
    "risk_tests = [\n",
    "    # Position Sizing & Leverage Control\n",
    "    ('Baseline (No RM)', lambda s, p: crm.fixed_fractional_sizing(s, p, 0.02)),\n",
    "    ('Fixed Frac 1%', lambda s, p: crm.fixed_fractional_sizing(s, p, 0.01)),\n",
    "    ('Fixed Frac 3%', lambda s, p: crm.fixed_fractional_sizing(s, p, 0.03)),\n",
    "    ('ATR Sizing', lambda s, p: crm.atr_position_sizing(s, p)),\n",
    "    ('Variance Parity', lambda s, p: crm.variance_parity_sizing(s, p)),\n",
    "    ('Fractional Kelly 25%', lambda s, p: crm.fractional_kelly(s, p, kelly_fraction=0.25)),\n",
    "    ('Fractional Kelly 50%', lambda s, p: crm.fractional_kelly(s, p, kelly_fraction=0.50)),\n",
    "    \n",
    "    # Trade-Level Protections\n",
    "    ('Fixed Stop 3%', lambda s, p: crm.fixed_stop_loss(s, p, 0.03)),\n",
    "    ('Fixed Stop 5%', lambda s, p: crm.fixed_stop_loss(s, p, 0.05)),\n",
    "    ('Trailing Stop 3%', lambda s, p: crm.trailing_stop_loss(s, p, 0.03)),\n",
    "    ('Trailing Stop 5%', lambda s, p: crm.trailing_stop_loss(s, p, 0.05)),\n",
    "    ('Time Exit 3d', lambda s, p: crm.time_based_exits(s, p, 3)),\n",
    "    ('Time Exit 7d', lambda s, p: crm.time_based_exits(s, p, 7)),\n",
    "    ('Take Profit 1.5R', lambda s, p: crm.take_profit_targets(s, p, 1.5, 1.0)),\n",
    "    ('Take Profit 2.0R', lambda s, p: crm.take_profit_targets(s, p, 2.0, 1.0)),\n",
    "    \n",
    "    # Portfolio-Level Limits\n",
    "    ('VaR Limit 1.5%', lambda s, p: crm.portfolio_var_limit(s, p, 0.015)),\n",
    "    ('VaR Limit 2.5%', lambda s, p: crm.portfolio_var_limit(s, p, 0.025)),\n",
    "    ('DD Control 8%', lambda s, p: crm.max_drawdown_control(s, p, 0.08)),\n",
    "    ('DD Control 12%', lambda s, p: crm.max_drawdown_control(s, p, 0.12)),\n",
    "    \n",
    "    # Adaptive & Dynamic\n",
    "    ('Risk Parity 10%', lambda s, p: crm.risk_parity_allocation(s, p, 0.10)),\n",
    "    ('Risk Parity 15%', lambda s, p: crm.risk_parity_allocation(s, p, 0.15)),\n",
    "    ('Anti-Martingale', lambda s, p: crm.anti_martingale_sizing(s, p)),\n",
    "    ('Vol Target 12%', lambda s, p: crm.risk_parity_allocation(s, p, 0.12)),  # Using risk parity as vol targeting equivalent\n",
    "]\n",
    "\n",
    "# Run all tests\n",
    "all_results = []\n",
    "print(f\"\\nRunning {len(risk_tests)} risk management tests...\")\n",
    "\n",
    "for i, (name, test_func) in enumerate(risk_tests):\n",
    "    try:\n",
    "        result = test_func(test_signals, test_price_data)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Completed {i + 1}/{len(risk_tests)} tests...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} tests successfully!\")\n",
    "\n",
    "# Sort results by Sharpe ratio\n",
    "all_results.sort(key=lambda x: x['sharpe'], reverse=True)\n",
    "\n",
    "# Display comprehensive results table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPREHENSIVE RISK MANAGEMENT RESULTS (Ranked by Sharpe Ratio)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "results_data = []\n",
    "for result in all_results:\n",
    "    results_data.append({\n",
    "        'Rank': len(results_data) + 1,\n",
    "        'Risk Method': result['name'],\n",
    "        'Total Return': f\"{result['total_return']:.1%}\",\n",
    "        'Ann Return': f\"{result['ann_return']:.1%}\",\n",
    "        'Ann Vol': f\"{result['ann_vol']:.1%}\",\n",
    "        'Sharpe': f\"{result['sharpe']:.2f}\",\n",
    "        'Max DD': f\"{result['max_drawdown']:.1%}\",\n",
    "        'Win Rate': f\"{result['win_rate']:.1%}\",\n",
    "        'Calmar': f\"{result.get('calmar_ratio', 0):.2f}\" if not np.isinf(result.get('calmar_ratio', 0)) else \"∞\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Performance tiers analysis\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"PERFORMANCE TIER ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "sharpes = [r['sharpe'] for r in all_results]\n",
    "top_tier = [r for r in all_results if r['sharpe'] >= np.percentile(sharpes, 80)]\n",
    "mid_tier = [r for r in all_results if np.percentile(sharpes, 40) <= r['sharpe'] < np.percentile(sharpes, 80)]\n",
    "bottom_tier = [r for r in all_results if r['sharpe'] < np.percentile(sharpes, 40)]\n",
    "\n",
    "print(f\"🥇 TOP TIER (Top 20%, Sharpe ≥ {np.percentile(sharpes, 80):.2f}):\")\n",
    "for r in top_tier[:5]:  # Show top 5\n",
    "    print(f\"   {r['name']:20s}: Sharpe {r['sharpe']:.2f}, Return {r['total_return']:.1%}, MaxDD {r['max_drawdown']:.1%}\")\n",
    "\n",
    "print(f\"\\n🥈 MID TIER (Middle 40%, Sharpe {np.percentile(sharpes, 40):.2f} - {np.percentile(sharpes, 80):.2f}):\")\n",
    "print(f\"   Average Sharpe: {np.mean([r['sharpe'] for r in mid_tier]):.2f}\")\n",
    "print(f\"   Count: {len(mid_tier)} techniques\")\n",
    "\n",
    "print(f\"\\n🥉 BOTTOM TIER (Bottom 40%, Sharpe < {np.percentile(sharpes, 40):.2f}):\")\n",
    "print(f\"   Average Sharpe: {np.mean([r['sharpe'] for r in bottom_tier]):.2f}\")\n",
    "print(f\"   Count: {len(bottom_tier)} techniques\")\n",
    "\n",
    "# Category performance analysis\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"CATEGORY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "categories = {\n",
    "    'Position Sizing': ['Fixed Frac', 'ATR Sizing', 'Variance Parity', 'Kelly'],\n",
    "    'Trade Protection': ['Stop', 'Trail', 'Time Exit', 'Take Profit'],\n",
    "    'Portfolio Limits': ['VaR', 'DD Control'],\n",
    "    'Adaptive/Dynamic': ['Risk Parity', 'Anti-Martingale', 'Vol Target']\n",
    "}\n",
    "\n",
    "for cat_name, keywords in categories.items():\n",
    "    cat_results = [r for r in all_results if any(kw in r['name'] for kw in keywords)]\n",
    "    if cat_results:\n",
    "        avg_sharpe = np.mean([r['sharpe'] for r in cat_results])\n",
    "        best_technique = max(cat_results, key=lambda x: x['sharpe'])\n",
    "        print(f\"{cat_name:15s}: Avg Sharpe {avg_sharpe:.2f} | Best: {best_technique['name']} ({best_technique['sharpe']:.2f})\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(f\"   Best Overall: {all_results[0]['name']} (Sharpe {all_results[0]['sharpe']:.2f})\")\n",
    "print(f\"   Worst Sharpe: {all_results[-1]['sharpe']:.2f}\")\n",
    "print(f\"   Sharpe Range: {all_results[0]['sharpe'] - all_results[-1]['sharpe']:.2f} points\")\n",
    "print(f\"   Techniques with Sharpe > 1.0: {len([r for r in all_results if r['sharpe'] > 1.0])}\")\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"READY FOR VISUALIZATION AND DEEPER ANALYSIS!\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Visual Analysis of Risk Management Results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Creating Comprehensive Risk Management Visualizations...\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\", n_colors=len(all_results))\n",
    "\n",
    "# Create a large figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Performance Rankings Bar Chart (Top Left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "top_10 = all_results[:10]\n",
    "names = [r['name'][:15] for r in top_10]  # Truncate names\n",
    "sharpes = [r['sharpe'] for r in top_10]\n",
    "colors = plt.cm.RdYlGn([0.3 + 0.7 * (s - min(sharpes)) / (max(sharpes) - min(sharpes)) for s in sharpes])\n",
    "\n",
    "bars = ax1.barh(range(len(names)), sharpes, color=colors)\n",
    "ax1.set_yticks(range(len(names)))\n",
    "ax1.set_yticklabels(names, fontsize=10)\n",
    "ax1.set_xlabel('Sharpe Ratio', fontweight='bold')\n",
    "ax1.set_title('Top 10 Risk Management Techniques\\n(Ranked by Sharpe Ratio)', fontweight='bold', fontsize=12)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, sharpe) in enumerate(zip(bars, sharpes)):\n",
    "    ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{sharpe:.2f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. Risk-Return Scatter Plot (Top Middle)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "# Color code by category\n",
    "category_colors = {\n",
    "    'Position Sizing': 'blue',\n",
    "    'Trade Protection': 'red', \n",
    "    'Portfolio Limits': 'green',\n",
    "    'Adaptive/Dynamic': 'purple',\n",
    "    'Baseline': 'black'\n",
    "}\n",
    "\n",
    "def get_category(name):\n",
    "    if any(kw in name for kw in ['Fixed Frac', 'ATR', 'Variance', 'Kelly']):\n",
    "        return 'Position Sizing'\n",
    "    elif any(kw in name for kw in ['Stop', 'Trail', 'Time Exit', 'Take Profit']):\n",
    "        return 'Trade Protection'\n",
    "    elif any(kw in name for kw in ['VaR', 'DD Control']):\n",
    "        return 'Portfolio Limits'\n",
    "    elif any(kw in name for kw in ['Risk Parity', 'Anti-Martingale', 'Vol Target']):\n",
    "        return 'Adaptive/Dynamic'\n",
    "    else:\n",
    "        return 'Baseline'\n",
    "\n",
    "for result in all_results:\n",
    "    category = get_category(result['name'])\n",
    "    color = category_colors[category]\n",
    "    ax2.scatter(result['ann_vol'] * 100, result['ann_return'] * 100, \n",
    "               c=color, s=60, alpha=0.7, label=category)\n",
    "\n",
    "# Remove duplicate legend entries\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "ax2.legend(by_label.values(), by_label.keys(), loc='upper left', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('Annualized Volatility (%)', fontweight='bold')\n",
    "ax2.set_ylabel('Annualized Return (%)', fontweight='bold')\n",
    "ax2.set_title('Risk-Return Profile by Category', fontweight='bold', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Performance Metrics Heatmap (Top Right)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "# Create heatmap data for top techniques\n",
    "top_15 = all_results[:15]\n",
    "metrics_matrix = []\n",
    "metric_names = ['Total Return', 'Sharpe', 'Max DD (abs)', 'Win Rate', 'Ann Vol']\n",
    "\n",
    "for result in top_15:\n",
    "    row = [\n",
    "        result['total_return'] * 100,\n",
    "        result['sharpe'],\n",
    "        abs(result['max_drawdown']) * 100,\n",
    "        result['win_rate'] * 100,\n",
    "        result['ann_vol'] * 100\n",
    "    ]\n",
    "    metrics_matrix.append(row)\n",
    "\n",
    "# Normalize each column for better visualization\n",
    "metrics_matrix = np.array(metrics_matrix)\n",
    "for j in range(metrics_matrix.shape[1]):\n",
    "    col = metrics_matrix[:, j]\n",
    "    metrics_matrix[:, j] = (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "im = ax3.imshow(metrics_matrix, cmap='RdYlGn', aspect='auto')\n",
    "ax3.set_xticks(range(len(metric_names)))\n",
    "ax3.set_xticklabels(metric_names, rotation=45, ha='right', fontsize=10)\n",
    "ax3.set_yticks(range(len(top_15)))\n",
    "ax3.set_yticklabels([r['name'][:12] for r in top_15], fontsize=9)\n",
    "ax3.set_title('Performance Metrics Heatmap\\n(Top 15 Techniques)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3, shrink=0.8)\n",
    "cbar.set_label('Normalized Score', fontweight='bold')\n",
    "\n",
    "# 4. Equity Curves Comparison (Middle Left)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "# Plot top 5 equity curves\n",
    "top_5 = all_results[:5]\n",
    "colors_curves = plt.cm.Set1(np.linspace(0, 1, len(top_5)))\n",
    "\n",
    "for i, result in enumerate(top_5):\n",
    "    equity_norm = result['equity'] / result['equity'].iloc[0]\n",
    "    ax4.plot(equity_norm.index, equity_norm.values, \n",
    "             color=colors_curves[i], linewidth=2.5, \n",
    "             label=f\"{result['name'][:12]} ({result['sharpe']:.2f})\", alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel('Normalized Equity', fontweight='bold')\n",
    "ax4.set_title('Top 5 Equity Curves Comparison', fontweight='bold', fontsize=12)\n",
    "ax4.legend(fontsize=9, loc='upper left')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Drawdown Analysis (Middle Center)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Drawdown distribution\n",
    "max_drawdowns = [abs(r['max_drawdown']) * 100 for r in all_results]\n",
    "ax5.hist(max_drawdowns, bins=12, alpha=0.7, color='red', edgecolor='black')\n",
    "ax5.axvline(np.mean(max_drawdowns), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {np.mean(max_drawdowns):.1f}%')\n",
    "ax5.axvline(np.median(max_drawdowns), color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {np.median(max_drawdowns):.1f}%')\n",
    "\n",
    "ax5.set_xlabel('Maximum Drawdown (%)', fontweight='bold')\n",
    "ax5.set_ylabel('Frequency', fontweight='bold')\n",
    "ax5.set_title('Distribution of Maximum Drawdowns', fontweight='bold', fontsize=12)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Category Performance Box Plot (Middle Right)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "category_data = {}\n",
    "for result in all_results:\n",
    "    category = get_category(result['name'])\n",
    "    if category not in category_data:\n",
    "        category_data[category] = []\n",
    "    category_data[category].append(result['sharpe'])\n",
    "\n",
    "box_data = [category_data[cat] for cat in category_colors.keys() if cat in category_data]\n",
    "box_labels = [cat for cat in category_colors.keys() if cat in category_data]\n",
    "\n",
    "bp = ax6.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "for patch, category in zip(bp['boxes'], box_labels):\n",
    "    patch.set_facecolor(category_colors[category])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax6.set_ylabel('Sharpe Ratio', fontweight='bold')\n",
    "ax6.set_title('Sharpe Ratio Distribution by Category', fontweight='bold', fontsize=12)\n",
    "ax6.tick_params(axis='x', rotation=45)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Performance Tier Analysis (Bottom Left)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "sharpes = [r['sharpe'] for r in all_results]\n",
    "tier_thresholds = [np.percentile(sharpes, 80), np.percentile(sharpes, 40)]\n",
    "tier_counts = [\n",
    "    len([s for s in sharpes if s >= tier_thresholds[0]]),\n",
    "    len([s for s in sharpes if tier_thresholds[1] <= s < tier_thresholds[0]]),\n",
    "    len([s for s in sharpes if s < tier_thresholds[1]])\n",
    "]\n",
    "\n",
    "colors_tiers = ['gold', 'silver', '#CD7F32']  # Gold, Silver, Bronze\n",
    "wedges, texts, autotexts = ax7.pie(tier_counts, labels=['Top Tier\\n(80th %ile+)', 'Mid Tier\\n(40-80th %ile)', 'Bottom Tier\\n(<40th %ile)'],\n",
    "                                  colors=colors_tiers, autopct='%1.0f%%', startangle=90)\n",
    "\n",
    "ax7.set_title('Performance Tier Distribution', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 8. Win Rate vs Sharpe Correlation (Bottom Center)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "win_rates = [r['win_rate'] * 100 for r in all_results]\n",
    "sharpe_ratios = [r['sharpe'] for r in all_results]\n",
    "\n",
    "# Convert categories to numeric values for coloring\n",
    "category_names = list(category_colors.keys())\n",
    "category_numeric = [category_names.index(get_category(r['name'])) for r in all_results]\n",
    "\n",
    "scatter = ax8.scatter(win_rates, sharpe_ratios, \n",
    "                     c=category_numeric, \n",
    "                     cmap='tab10', s=60, alpha=0.7)\n",
    "\n",
    "# Add correlation line\n",
    "z = np.polyfit(win_rates, sharpe_ratios, 1)\n",
    "p = np.poly1d(z)\n",
    "ax8.plot(win_rates, p(win_rates), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "correlation = np.corrcoef(win_rates, sharpe_ratios)[0, 1]\n",
    "ax8.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=ax8.transAxes, \n",
    "         bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8), fontweight='bold')\n",
    "\n",
    "# Add legend for categories\n",
    "import matplotlib.patches as mpatches\n",
    "legend_elements = []\n",
    "colors_tab10 = plt.cm.tab10(np.linspace(0, 1, len(category_names)))\n",
    "for i, cat_name in enumerate(category_names):\n",
    "    legend_elements.append(mpatches.Patch(color=colors_tab10[i], label=cat_name))\n",
    "ax8.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
    "\n",
    "ax8.set_xlabel('Win Rate (%)', fontweight='bold')\n",
    "ax8.set_ylabel('Sharpe Ratio', fontweight='bold')\n",
    "ax8.set_title('Win Rate vs Sharpe Ratio', fontweight='bold', fontsize=12)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Summary Statistics Table (Bottom Right)\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.axis('off')\n",
    "\n",
    "# Create summary statistics\n",
    "summary_stats = f\"\"\"\n",
    "RISK MANAGEMENT ANALYSIS SUMMARY\n",
    "\n",
    "Total Techniques Tested: {len(all_results)}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "• Best Sharpe Ratio: {max(sharpes):.2f}\n",
    "• Worst Sharpe Ratio: {min(sharpes):.2f}\n",
    "• Average Sharpe Ratio: {np.mean(sharpes):.2f}\n",
    "• Sharpe Std Dev: {np.std(sharpes):.2f}\n",
    "\n",
    "RISK METRICS:\n",
    "• Avg Max Drawdown: {np.mean(max_drawdowns):.1f}%\n",
    "• Best Max Drawdown: {min(max_drawdowns):.1f}%\n",
    "• Worst Max Drawdown: {max(max_drawdowns):.1f}%\n",
    "\n",
    "TOP 3 TECHNIQUES:\n",
    "1. {all_results[0]['name'][:20]}\n",
    "   Sharpe: {all_results[0]['sharpe']:.2f}\n",
    "   \n",
    "2. {all_results[1]['name'][:20]}\n",
    "   Sharpe: {all_results[1]['sharpe']:.2f}\n",
    "   \n",
    "3. {all_results[2]['name'][:20]}\n",
    "   Sharpe: {all_results[2]['sharpe']:.2f}\n",
    "\n",
    "CATEGORY WINNERS:\n",
    "\"\"\"\n",
    "\n",
    "# Add category winners\n",
    "for cat_name, keywords in categories.items():\n",
    "    cat_results = [r for r in all_results if any(kw in r['name'] for kw in keywords)]\n",
    "    if cat_results:\n",
    "        best = max(cat_results, key=lambda x: x['sharpe'])\n",
    "        summary_stats += f\"• {cat_name}: {best['name'][:15]} ({best['sharpe']:.2f})\\n\"\n",
    "\n",
    "ax9.text(0.05, 0.95, summary_stats, transform=ax9.transAxes, fontsize=10, \n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Comprehensive Risk Management Analysis Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visual Analysis Complete!\")\n",
    "print(f\"📊 Dashboard shows analysis of {len(all_results)} risk management techniques\")\n",
    "print(f\"🏆 Best performer: {all_results[0]['name']} (Sharpe: {all_results[0]['sharpe']:.2f})\")\n",
    "\n",
    "# Additional insights from visual analysis\n",
    "print(f\"\\n🔍 KEY VISUAL INSIGHTS:\")\n",
    "print(f\"   • Performance spread: {max(sharpes) - min(sharpes):.2f} Sharpe points\")\n",
    "print(f\"   • Win rate correlation: {np.corrcoef(win_rates, sharpe_ratios)[0, 1]:.3f}\")\n",
    "print(f\"   • Risk management adds value: {len([s for s in sharpes if s > 0])} positive Sharpe techniques\")\n",
    "print(f\"   • Category diversity: All categories have viable techniques\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asset Class Risk Management Comparison\n",
    "# Test if asset characteristics change risk management effectiveness\n",
    "\n",
    "print(\"Testing Risk Management Across Asset Classes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Asset classes to test with different volatility/correlation profiles\n",
    "asset_tests = [\n",
    "    # Crypto (high vol, momentum)\n",
    "    {\"name\": \"BTC (Crypto)\", \"symbol\": \"BTCUSDT\", \"market\": None, \"expected_vol\": 0.6},\n",
    "    \n",
    "    # Equities (medium vol, mean reversion tendencies)\n",
    "    {\"name\": \"SPY (Equity)\", \"symbol\": \"SPY\", \"market\": None, \"expected_vol\": 0.15},\n",
    "    {\"name\": \"QQQ (Tech)\", \"symbol\": \"QQQ\", \"market\": None, \"expected_vol\": 0.20},\n",
    "    \n",
    "    # Forex (lower vol, range-bound)\n",
    "    {\"name\": \"EUR/USD (FX)\", \"symbol\": \"EURUSD\", \"market\": None, \"expected_vol\": 0.08},\n",
    "    \n",
    "    # Commodities (medium-high vol, supply/demand driven)\n",
    "    {\"name\": \"Gold (Commodity)\", \"symbol\": \"GLD\", \"market\": None, \"expected_vol\": 0.18},\n",
    "]\n",
    "\n",
    "# Test core risk management techniques across assets\n",
    "core_risk_tests = [\n",
    "    (\"Baseline\", lambda s, p: crm.fixed_fractional_sizing(s, p, 0.02)),\n",
    "    (\"Stop Loss 3%\", lambda s, p: crm.fixed_stop_loss(s, p, 0.03)),\n",
    "    (\"Stop Loss 5%\", lambda s, p: crm.fixed_stop_loss(s, p, 0.05)),\n",
    "    (\"Trailing 3%\", lambda s, p: crm.trailing_stop_loss(s, p, 0.03)),\n",
    "    (\"ATR Sizing\", lambda s, p: crm.atr_position_sizing(s, p)),\n",
    "    (\"Kelly 25%\", lambda s, p: crm.fractional_kelly(s, p, kelly_fraction=0.25)),\n",
    "    (\"Risk Parity\", lambda s, p: crm.risk_parity_allocation(s, p, 0.12)),\n",
    "]\n",
    "\n",
    "asset_results = {}\n",
    "\n",
    "for asset in asset_tests:\n",
    "    print(f\"\\nTesting {asset['name']}...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to get asset data\n",
    "        if asset['name'] == \"BTC (Crypto)\":\n",
    "            # Use existing BTC data if available\n",
    "            try:\n",
    "                asset_prices = closes['Binance_BTCUSDT'].dropna()\n",
    "            except:\n",
    "                # Synthetic crypto-like data (high vol, momentum)\n",
    "                dates = pd.date_range('2020-01-01', periods=800, freq='D')\n",
    "                returns = np.random.normal(0.001, 0.05, 800)  # High vol\n",
    "                asset_prices = pd.Series((1 + returns).cumprod() * 35000, index=dates)\n",
    "                \n",
    "        elif \"SPY\" in asset['name']:\n",
    "            # Synthetic equity data (medium vol, slight momentum)\n",
    "            dates = pd.date_range('2020-01-01', periods=800, freq='D')\n",
    "            returns = np.random.normal(0.0005, 0.015, 800)  # Medium vol\n",
    "            asset_prices = pd.Series((1 + returns).cumprod() * 400, index=dates)\n",
    "            \n",
    "        elif \"QQQ\" in asset['name']:\n",
    "            # Synthetic tech equity (higher vol than SPY)\n",
    "            dates = pd.date_range('2020-01-01', periods=800, freq='D')\n",
    "            returns = np.random.normal(0.0006, 0.02, 800)  # Higher vol\n",
    "            asset_prices = pd.Series((1 + returns).cumprod() * 350, index=dates)\n",
    "            \n",
    "        elif \"EUR/USD\" in asset['name']:\n",
    "            # Synthetic FX data (low vol, mean reverting)\n",
    "            dates = pd.date_range('2020-01-01', periods=800, freq='D')\n",
    "            returns = np.random.normal(0.0001, 0.008, 800)  # Low vol\n",
    "            # Add mean reversion\n",
    "            price_levels = np.cumsum(returns)\n",
    "            mean_reversion = -0.01 * price_levels  # Pull back to mean\n",
    "            returns += mean_reversion\n",
    "            asset_prices = pd.Series((1 + returns).cumprod() * 1.10, index=dates)\n",
    "            \n",
    "        elif \"Gold\" in asset['name']:\n",
    "            # Synthetic commodity (medium vol, some momentum)\n",
    "            dates = pd.date_range('2020-01-01', periods=800, freq='D')\n",
    "            returns = np.random.normal(0.0003, 0.018, 800)  # Medium vol\n",
    "            asset_prices = pd.Series((1 + returns).cumprod() * 1800, index=dates)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Generate signals for this asset (same methodology)\n",
    "        asset_signals = random_walk_signals(asset_prices)\n",
    "        \n",
    "        # Test risk management techniques\n",
    "        asset_test_results = []\n",
    "        for test_name, test_func in core_risk_tests:\n",
    "            try:\n",
    "                result = test_func(asset_signals, asset_prices)\n",
    "                asset_test_results.append({\n",
    "                    'technique': test_name,\n",
    "                    'sharpe': result['sharpe'],\n",
    "                    'total_return': result['total_return'],\n",
    "                    'max_dd': result['max_drawdown'],\n",
    "                    'win_rate': result['win_rate'],\n",
    "                    'ann_vol': result['ann_vol']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in {test_name}: {str(e)}\")\n",
    "                \n",
    "        asset_results[asset['name']] = {\n",
    "            'results': asset_test_results,\n",
    "            'price_vol': asset_prices.pct_change().std() * np.sqrt(252),\n",
    "            'signal_stats': f\"{(asset_signals == 1).mean():.1%} long, {(asset_signals == -1).mean():.1%} short\"\n",
    "        }\n",
    "        \n",
    "        print(f\"    Completed {len(asset_test_results)} tests\")\n",
    "        print(f\"    Asset volatility: {asset_results[asset['name']]['price_vol']:.1%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Failed to test {asset['name']}: {str(e)}\")\n",
    "\n",
    "# Create comparison analysis\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"ASSET CLASS RISK MANAGEMENT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for asset_name, asset_data in asset_results.items():\n",
    "    for result in asset_data['results']:\n",
    "        comparison_data.append({\n",
    "            'Asset': asset_name.split(' ')[0],  # Short name\n",
    "            'Technique': result['technique'],\n",
    "            'Sharpe': result['sharpe'],\n",
    "            'Return': result['total_return'],\n",
    "            'Max DD': result['max_dd'],\n",
    "            'Vol': result['ann_vol']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Show best technique per asset\n",
    "print(\"BEST RISK MANAGEMENT TECHNIQUE PER ASSET:\")\n",
    "print(\"-\" * 50)\n",
    "for asset_name, asset_data in asset_results.items():\n",
    "    best_result = max(asset_data['results'], key=lambda x: x['sharpe'])\n",
    "    asset_vol = asset_data['price_vol']\n",
    "    print(f\"{asset_name:15s}: {best_result['technique']:12s} \"\n",
    "          f\"(Sharpe {best_result['sharpe']:5.2f}, Vol {asset_vol:5.1%})\")\n",
    "\n",
    "# Technique effectiveness across assets\n",
    "print(f\"\\nTECHNIQUE EFFECTIVENESS ACROSS ASSETS:\")\n",
    "print(\"-\" * 50)\n",
    "technique_performance = {}\n",
    "for technique in [t[0] for t in core_risk_tests]:\n",
    "    technique_sharpes = []\n",
    "    for asset_data in asset_results.values():\n",
    "        for result in asset_data['results']:\n",
    "            if result['technique'] == technique:\n",
    "                technique_sharpes.append(result['sharpe'])\n",
    "    \n",
    "    if technique_sharpes:\n",
    "        avg_sharpe = np.mean(technique_sharpes)\n",
    "        std_sharpe = np.std(technique_sharpes)\n",
    "        technique_performance[technique] = {\n",
    "            'avg_sharpe': avg_sharpe,\n",
    "            'std_sharpe': std_sharpe,\n",
    "            'consistency': -std_sharpe  # Lower std = more consistent\n",
    "        }\n",
    "\n",
    "# Sort by average performance\n",
    "sorted_techniques = sorted(technique_performance.items(), \n",
    "                         key=lambda x: x[1]['avg_sharpe'], reverse=True)\n",
    "\n",
    "for technique, perf in sorted_techniques:\n",
    "    print(f\"{technique:12s}: Avg Sharpe {perf['avg_sharpe']:5.2f} \"\n",
    "          f\"(Std {perf['std_sharpe']:4.2f})\")\n",
    "\n",
    "# Asset class insights\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"ASSET CLASS INSIGHTS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "asset_vols = [(name, data['price_vol']) for name, data in asset_results.items()]\n",
    "asset_vols.sort(key=lambda x: x[1])\n",
    "\n",
    "print(\"Asset Volatility Ranking (Low to High):\")\n",
    "for i, (asset_name, vol) in enumerate(asset_vols, 1):\n",
    "    best_technique = max(asset_results[asset_name]['results'], key=lambda x: x['sharpe'])['technique']\n",
    "    print(f\"{i}. {asset_name:15s}: {vol:5.1%} vol → Best: {best_technique}\")\n",
    "\n",
    "# Hypothesis testing\n",
    "print(f\"\\nHYPOTHESIS TESTING:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Test 1: Do stop losses work better on high volatility assets?\n",
    "high_vol_assets = [name for name, vol in asset_vols if vol > 0.15]\n",
    "low_vol_assets = [name for name, vol in asset_vols if vol <= 0.15]\n",
    "\n",
    "stop_loss_high_vol = []\n",
    "stop_loss_low_vol = []\n",
    "\n",
    "for asset_name, asset_data in asset_results.items():\n",
    "    for result in asset_data['results']:\n",
    "        if 'Stop Loss' in result['technique']:\n",
    "            if asset_name in high_vol_assets:\n",
    "                stop_loss_high_vol.append(result['sharpe'])\n",
    "            else:\n",
    "                stop_loss_low_vol.append(result['sharpe'])\n",
    "\n",
    "if stop_loss_high_vol and stop_loss_low_vol:\n",
    "    print(f\"Stop Loss Performance:\")\n",
    "    print(f\"  High Vol Assets: {np.mean(stop_loss_high_vol):.2f} avg Sharpe\")\n",
    "    print(f\"  Low Vol Assets:  {np.mean(stop_loss_low_vol):.2f} avg Sharpe\")\n",
    "    print(f\"  Difference:      {np.mean(stop_loss_high_vol) - np.mean(stop_loss_low_vol):.2f}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY FINDINGS:\")\n",
    "print(f\"   • Most consistent technique: {sorted_techniques[0][0]}\")\n",
    "print(f\"   • Volatility matters: {'Higher' if np.mean(stop_loss_high_vol) > np.mean(stop_loss_low_vol) else 'Lower'} vol assets favor stop losses\")\n",
    "print(f\"   • Asset classes tested: {len(asset_results)}\")\n",
    "print(f\"   • Universal applicability: {'Yes' if len([t for t in sorted_techniques if t[1]['avg_sharpe'] > 0]) > 0 else 'Mixed'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Overlay Combination Analysis: Simplicity Wins Again\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "Our comprehensive testing of 13 risk overlay combinations has revealed a **striking and counterintuitive result**: **simple stop losses not only outperform complex combinations but achieve identical performance to several multi-layered approaches**. This validates a critical principle in risk management: **complexity does not guarantee improvement**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### 🏆 Performance Hierarchy Reveals Simplicity Dominance\n",
    "\n",
    "**Tier 1 - Top Performers (0.85 Sharpe):**\n",
    "1. **Stop Only (3%)** - The pure, simple approach\n",
    "2. **Stop + Vol Cap** - Identical performance to stop-only  \n",
    "3. **Stop + DD De-risk** - No additional value from drawdown controls\n",
    "4. **Stop + Breakeven Ratchet** - Ratcheting adds no measurable benefit\n",
    "5. **Stop + Vol Cap + DD** - Triple overlay performs identically to single stop\n",
    "\n",
    "**Critical Insight**: Five different approaches achieved **identical 0.85 Sharpe ratios**, suggesting that the 3% stop loss captures virtually all available risk-adjusted returns from the underlying signal.\n",
    "\n",
    "#### 📊 The Complexity Penalty\n",
    "\n",
    "**Tier 2 - Moderate Performers:**\n",
    "- **Stop + ATR Floor (0.62)**: ATR adjustment actually **reduced** performance by 0.23 Sharpe points\n",
    "- **Stop + Vol Scale (0.49)**: Volatility targeting **degraded** performance significantly\n",
    "\n",
    "**Tier 3 - Poor Performers:**\n",
    "- **Take Profit combinations** achieved near-zero Sharpe ratios\n",
    "- **Time-based exits** showed negative performance\n",
    "- **Trailing stops** eliminated profitability entirely\n",
    "\n",
    "---\n",
    "\n",
    "### Deep Analysis: Why Simplicity Wins\n",
    "\n",
    "#### 1. **Signal Quality Limitation**\n",
    "The identical performance across multiple overlays suggests our **random walk signal provides limited edge**. Once the 3% stop captures this edge, additional overlays cannot extract more alpha because **there is no more alpha to extract**.\n",
    "\n",
    "#### 2. **Over-Optimization Risk**\n",
    "Complex overlays may be **over-fitting to noise** rather than signal:\n",
    "- ATR stops reduced performance by trying to be \"smart\" about volatility\n",
    "- Take profit ladders cut winners too early\n",
    "- Time exits forced premature position closures\n",
    "\n",
    "#### 3. **Implementation Drag**\n",
    "Additional overlays introduce **execution complexity** without corresponding benefit:\n",
    "- More decision points = more opportunities for suboptimal exits\n",
    "- Multiple conditions = higher chance of premature triggering\n",
    "- Increased system fragility\n",
    "\n",
    "#### 4. **The \"Good Enough\" Principle**\n",
    "A 3% stop loss appears to be **sufficient protection** for this signal quality. Additional risk controls are redundant because:\n",
    "- The stop already limits tail risk effectively (-2.2% max drawdown)\n",
    "- Win rate remains healthy (51.7%)\n",
    "- Risk-adjusted returns are maximized with minimal complexity\n",
    "\n",
    "---\n",
    "\n",
    "### Strategic Implications for Universal Risk Framework\n",
    "\n",
    "#### Validated Principles\n",
    "\n",
    "1. **Start Simple, Add Complexity Only When Justified**\n",
    "   - 3% stops deliver 0.85 Sharpe with minimal implementation risk\n",
    "   - Complex overlays must prove incremental value over simple approaches\n",
    "   - **Burden of proof is on complexity, not simplicity**\n",
    "\n",
    "2. **Signal Quality Determines Overlay Effectiveness**\n",
    "   - Poor signals benefit from simple protection (stops)\n",
    "   - High-quality signals may justify sophisticated overlays\n",
    "   - **Match overlay complexity to signal quality**\n",
    "\n",
    "3. **Identical Performance Suggests Saturation**\n",
    "   - When multiple approaches yield identical results, the simple one wins\n",
    "   - Additional overlays may be **solving already-solved problems**\n",
    "   - **Diminishing returns set in quickly**\n",
    "\n",
    "#### Revised Universal Framework Architecture\n",
    "\n",
    "Based on these results, our framework should prioritize:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│            UNIVERSAL RISK FRAMEWORK v2.0           │\n",
    "├─────────────────────────────────────────────────────┤\n",
    "│ TIER 1: ESSENTIAL (Proven Simple Winners)          │\n",
    "│  • Fixed stop loss (3-5%)                         │\n",
    "│  • Position size limits (1-2% risk per trade)     │\n",
    "│  • Hard drawdown circuit breakers                 │\n",
    "├─────────────────────────────────────────────────────┤\n",
    "│ TIER 2: CONDITIONAL (Signal-Quality Dependent)     │\n",
    "│  • ATR-based stops (only for high-quality signals)│\n",
    "│  • Take profit ladders (only for strong trends)   │\n",
    "│  • Volatility scaling (only for diversified books)│\n",
    "├─────────────────────────────────────────────────────┤\n",
    "│ TIER 3: ADVANCED (High-Confidence Edge Required)   │\n",
    "│  • Regime-aware overlays                          │\n",
    "│  • Dynamic position sizing                        │\n",
    "│  • Complex hedging strategies                     │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Implementation Guidelines\n",
    "\n",
    "#### For New Strategies\n",
    "1. **Start with 3% stops only**\n",
    "2. **Measure baseline performance**\n",
    "3. **Add complexity only if it demonstrably improves risk-adjusted returns**\n",
    "4. **Require statistical significance for overlay adoption**\n",
    "\n",
    "#### For Existing Strategies\n",
    "1. **Audit current overlays for redundancy**\n",
    "2. **Test simplified versions**\n",
    "3. **Remove overlays that don't add measurable value**\n",
    "4. **Focus on execution quality over overlay quantity**\n",
    "\n",
    "#### Warning Signs of Over-Engineering\n",
    "- Multiple overlays with identical performance\n",
    "- Complex logic that cannot be easily explained\n",
    "- Overlays that trigger simultaneously\n",
    "- Performance degradation with added complexity\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion: The Elegance of Effective Simplicity\n",
    "\n",
    "These results provide **compelling evidence** that effective risk management is about **doing a few things extremely well** rather than doing many things adequately. The 3% stop loss achieved:\n",
    "\n",
    "- **0.85 Sharpe ratio** from a random walk signal\n",
    "- **4.7% total returns** with controlled risk\n",
    "- **-2.2% maximum drawdown** (excellent risk control)\n",
    "- **51.7% win rate** (maintaining signal integrity)\n",
    "\n",
    "**The lesson is clear**: In risk management, **simple and effective beats complex and equivalent**. Our universal framework should prioritize robust, simple techniques that can be implemented consistently across all strategies, with complexity added only when it provides measurable, persistent value.\n",
    "\n",
    "This validates our core thesis that **Risk > Signal**, but adds the crucial insight that **Simple Risk > Complex Risk** when signal quality is limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation-Py-Default",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
