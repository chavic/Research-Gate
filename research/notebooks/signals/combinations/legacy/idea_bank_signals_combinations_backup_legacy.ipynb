{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["# Signal Combinations Framework\n","\n","Compact implementation of multi-signal trading strategies:\n","- **Regime-Aware**: Adapt to market conditions\n","- **Confirmation**: Multiple signals must agree  \n","- **Ensemble**: Weighted voting systems\n","- **Hierarchical**: Filtered approaches"]},{"cell_type":"code","execution_count":1,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime, timedelta\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","try:\n","    from AlgorithmImports import *\n","    qb = QuantBook()\n","except:\n","    qb = None\n","\n","np.random.seed(42)\n","print(\"Signal Combinations Framework Ready\")"]},{"cell_type":"markdown","metadata":{},"source":["class SignalCombinations:\n","    \"\"\"Compact signal combination strategies\"\"\"\n","    \n","    def __init__(self, signals_dict, data):\n","        self.signals = signals_dict\n","        self.data = data\n","        self.combinations = {}\n","    \n","    def _get_signals(self, signal_names):\n","        \"\"\"Get signals by name, return empty list if not found\"\"\"\n","        return [self.signals[name] for name in signal_names if name in self.signals]\n","    \n","    def _consensus(self, signals, threshold=0.5):\n","        \"\"\"Calculate consensus from multiple signals\"\"\"\n","        if not signals: return np.zeros(len(self.data))\n","        votes = np.mean(signals, axis=0)\n","        return np.where(votes > threshold, 1, np.where(votes < -threshold, -1, 0))\n","    \n","    def volatility_regime(self):\n","        \"\"\"Use momentum in high vol, mean reversion in low vol\"\"\"\n","        returns = self.data['close'].pct_change()\n","        vol_regime = returns.rolling(20).std() > returns.rolling(100).std()\n","        \n","        momentum_sigs = self._get_signals(['Fractal_Hurst', 'KAMA', 'Donchian_Breakout'])\n","        mean_rev_sigs = self._get_signals(['VWAP_ZScore', 'BB_Squeeze', 'Gap_Fill'])\n","        \n","        combined = np.zeros(len(self.data))\n","        if momentum_sigs and mean_rev_sigs:\n","            momentum_consensus = self._consensus(momentum_sigs, 0.3)\n","            mean_rev_consensus = self._consensus(mean_rev_sigs, 0.3)\n","            combined = np.where(vol_regime, momentum_consensus, mean_rev_consensus)\n","        \n","        self.combinations['Volatility_Regime'] = combined\n","        return combined\n","    \n","    def momentum_confirmation(self):\n","        \"\"\"Require 70% agreement from momentum signals\"\"\"\n","        momentum_sigs = self._get_signals(['Fractal_Hurst', 'KAMA', 'Donchian_Breakout', 'ML_GradientBoosting'])\n","        combined = self._consensus(momentum_sigs, 0.7) if len(momentum_sigs) >= 3 else np.zeros(len(self.data))\n","        self.combinations['Momentum_Confirmation'] = combined\n","        return combined\n","    \n","    def ensemble_weighted(self):\n","        \"\"\"Weight signals by performance\"\"\"\n","        returns = self.data['close'].pct_change().shift(-1)\n","        weights = {}\n","        \n","        # Calculate weights based on signal performance\n","        for name, signals in self.signals.items():\n","            if name == 'Random_Baseline': continue\n","            signal_returns = [signals[i] * returns.iloc[i] for i in range(len(signals)) if signals[i] != 0 and i < len(returns)-1]\n","            if len(signal_returns) > 10:\n","                sharpe = np.mean(signal_returns) / np.std(signal_returns) if np.std(signal_returns) > 0 else 0\n","                weights[name] = max(0, sharpe)\n","        \n","        # Normalize weights\n","        total_weight = sum(weights.values())\n","        if total_weight > 0:\n","            weights = {k: v/total_weight for k, v in weights.items()}\n","        \n","        # Combine with weights\n","        combined = np.zeros(len(self.data))\n","        for i in range(len(self.data)):\n","            weighted_vote = sum(weights.get(name, 0) * signals[i] for name, signals in self.signals.items())\n","            combined[i] = 1 if weighted_vote > 0.3 else -1 if weighted_vote < -0.3 else 0\n","        \n","        self.combinations['Ensemble_Weighted'] = combined\n","        return combined\n","    \n","    def hierarchical_filter(self):\n","        \"\"\"Multi-level filtering approach\"\"\"\n","        # Primary signals\n","        primary_sigs = self._get_signals(['ML_GradientBoosting', 'RV_Regime'])\n","        if not primary_sigs: return np.zeros(len(self.data))\n","        \n","        primary_consensus = self._consensus(primary_sigs, 0.4)\n","        \n","        # Volume filter\n","        if 'volume' in self.data.columns:\n","            vol_ma = self.data['volume'].rolling(20).mean()\n","            low_volume = self.data['volume'] < vol_ma * 0.7\n","            primary_consensus = np.where(low_volume, 0, primary_consensus)\n","        \n","        self.combinations['Hierarchical_Filter'] = primary_consensus\n","        return primary_consensus\n","    \n","    def run_all_strategies(self):\n","        \"\"\"Execute all combination strategies\"\"\"\n","        strategies = [\n","            self.volatility_regime,\n","            self.momentum_confirmation,\n","            self.ensemble_weighted,\n","            self.hierarchical_filter\n","        ]\n","        \n","        for strategy in strategies:\n","            try:\n","                strategy()\n","                print(f\"{strategy.__name__} - Done\")\n","            except Exception as e:\n","                print(f\"{strategy.__name__} failed: {e}\")\n","        \n","        # Add random baseline\n","        self.combinations['Random_Baseline'] = np.random.choice([-1, 0, 1], size=len(self.data), p=[0.3, 0.4, 0.3])\n","        \n","        return self.combinations\n","\n","print(\"SignalCombinations class ready\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyze_performance(data, signals_dict):\n","    \"\"\"Compact performance analysis\"\"\"\n","    returns = data['close'].pct_change().shift(-1)\n","    results = []\n","    \n","    for name, signals in signals_dict.items():\n","        signal_returns = np.where(signals != 0, signals * returns, 0)\n","        valid_returns = signal_returns[signals != 0]\n","        \n","        if len(valid_returns) > 5:\n","            total_return = np.sum(signal_returns)\n","            avg_return = np.mean(valid_returns)\n","            volatility = np.std(valid_returns)\n","            sharpe = avg_return / volatility if volatility > 0 else 0\n","            frequency = np.sum(signals != 0) / len(signals)\n","            \n","            results.append({\n","                'strategy': name,\n","                'sharpe': sharpe,\n","                'total_return': total_return,\n","                'frequency': frequency,\n","                'signals': np.sum(signals != 0)\n","            })\n","    \n","    df = pd.DataFrame(results).sort_values('sharpe', ascending=False)\n","    return df\n","\n","print(\"Performance analyzer ready\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Demo execution with sample data\n","def run_demo():\n","    \"\"\"Run demonstration with sample data\"\"\"\n","    # Create sample data\n","    dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')\n","    prices = 50000 * np.cumprod(1 + np.random.normal(0.001, 0.02, len(dates)))\n","    \n","    data = pd.DataFrame({\n","        'close': prices,\n","        'high': prices * (1 + np.abs(np.random.normal(0, 0.01, len(dates)))),\n","        'low': prices * (1 - np.abs(np.random.normal(0, 0.01, len(dates)))),\n","        'volume': np.random.lognormal(10, 0.5, len(dates))\n","    }, index=dates)\n","    \n","    # Create sample signals\n","    sample_signals = {\n","        'Fractal_Hurst': np.random.choice([-1, 0, 1], len(data), p=[0.2, 0.6, 0.2]),\n","        'KAMA': np.random.choice([-1, 0, 1], len(data), p=[0.25, 0.5, 0.25]),\n","        'Donchian_Breakout': np.random.choice([-1, 0, 1], len(data), p=[0.15, 0.7, 0.15]),\n","        'VWAP_ZScore': np.random.choice([-1, 0, 1], len(data), p=[0.2, 0.6, 0.2]),\n","        'BB_Squeeze': np.random.choice([-1, 0, 1], len(data), p=[0.1, 0.8, 0.1]),\n","        'ML_GradientBoosting': np.random.choice([-1, 0, 1], len(data), p=[0.2, 0.6, 0.2]),\n","        'RV_Regime': np.random.choice([-1, 0, 1], len(data), p=[0.3, 0.4, 0.3]),\n","        'Gap_Fill': np.random.choice([-1, 0, 1], len(data), p=[0.1, 0.8, 0.1])\n","    }\n","    \n","    # Run combinations\n","    combiner = SignalCombinations(sample_signals, data)\n","    all_combinations = combiner.run_all_strategies()\n","    \n","    # Analyze performance\n","    performance = analyze_performance(data, all_combinations)\n","    \n","    print(f\"\\nResults for {len(all_combinations)} strategies:\")\n","    print(performance.head(10).to_string(index=False))\n","    \n","    return data, all_combinations, performance\n","\n","# Run demo if executed\n","if __name__ == \"__main__\":\n","    data, combinations, results = run_demo()\n","    print(\"\\nDemo completed successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Usage Instructions\n","\n","### Quick Start:\n","```python\n","# Initialize with your signals and data\n","combiner = SignalCombinations(your_signals_dict, your_data)\n","results = combiner.run_all_strategies()\n","\n","# Analyze performance\n","performance = analyze_performance(your_data, results)\n","print(performance)\n","```\n","\n","### Strategies Included:\n","1. **Volatility Regime**: Momentum in high vol, mean reversion in low vol\n","2. **Momentum Confirmation**: 70% agreement threshold\n","3. **Ensemble Weighted**: Performance-based signal weighting\n","4. **Hierarchical Filter**: Multi-level filtering with volume confirmation\n","\n","### Key Features:\n","- Compact implementation (~150 lines vs 1800+)\n","- Essential functionality preserved\n","- Easy to extend and modify\n","- Built-in performance analysis\n","- Sample data demo included\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Ensemble & Weighted Voting Systems\n","\n","Advanced ensemble methods with dynamic weighting based on signal performance.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class EnsembleStrategies:\n","    \"\"\"\n","    Ensemble & Weighted Voting Systems\n","    Core Concept: Weight signals based on historical performance and combine intelligently\n","    \"\"\"\n","    \n","    def __init__(self, signals_dict, data):\n","        self.signals = signals_dict\n","        self.data = data\n","        self.combinations = {}\n","        self.signal_weights = {}\n","    \n","    def calculate_signal_weights(self, lookback_window=100):\n","        \"\"\"\n","        Calculate dynamic weights based on recent signal performance\n","        \"\"\"\n","        returns = self.data['close'].pct_change().shift(-1)  # Next period returns\n","        \n","        for signal_name, signals in self.signals.items():\n","            if signal_name == 'Random_Baseline':\n","                continue\n","                \n","            # Calculate signal returns\n","            signal_returns = []\n","            for i in range(len(signals)):\n","                if signals[i] != 0 and i < len(returns) - 1:\n","                    signal_returns.append(signals[i] * returns.iloc[i])\n","            \n","            if len(signal_returns) > 10:\n","                # Recent performance (last lookback_window periods)\n","                recent_returns = signal_returns[-lookback_window:] if len(signal_returns) > lookback_window else signal_returns\n","                \n","                # Weight based on Sharpe ratio\n","                mean_return = np.mean(recent_returns)\n","                std_return = np.std(recent_returns)\n","                sharpe = mean_return / std_return if std_return > 0 else 0\n","                \n","                # Normalize weight (positive sharpe gets higher weight)\n","                self.signal_weights[signal_name] = max(0, sharpe)\n","            else:\n","                self.signal_weights[signal_name] = 0.1  # Default small weight\n","        \n","        # Normalize weights to sum to 1\n","        total_weight = sum(self.signal_weights.values())\n","        if total_weight > 0:\n","            for signal_name in self.signal_weights:\n","                self.signal_weights[signal_name] /= total_weight\n","    \n","    def adaptive_weighted_ensemble(self):\n","        \"\"\"\n","        Adaptive Weighted Ensemble\n","        Dynamically weight signals based on rolling performance\n","        \"\"\"\n","        self.calculate_signal_weights()\n","        \n","        combined_signals = np.zeros(len(self.data))\n","        \n","        for i in range(len(self.data)):\n","            weighted_vote = 0\n","            total_weight = 0\n","            \n","            for signal_name, signals in self.signals.items():\n","                if signal_name in self.signal_weights and signals[i] != 0:\n","                    weight = self.signal_weights[signal_name]\n","                    weighted_vote += weight * signals[i]\n","                    total_weight += weight\n","            \n","            # Convert to discrete signal\n","            if total_weight > 0:\n","                avg_vote = weighted_vote / total_weight\n","                if avg_vote > 0.3:\n","                    combined_signals[i] = 1\n","                elif avg_vote < -0.3:\n","                    combined_signals[i] = -1\n","        \n","        self.combinations['Adaptive_Weighted_Ensemble'] = combined_signals\n","        return combined_signals\n","    \n","    def confidence_weighted_ensemble(self):\n","        \"\"\"\n","        Confidence-Weighted Ensemble\n","        Weight signals based on their confidence/strength\n","        \"\"\"\n","        combined_signals = np.zeros(len(self.data))\n","        \n","        for i in range(len(self.data)):\n","            votes = []\n","            confidences = []\n","            \n","            for signal_name, signals in self.signals.items():\n","                if signal_name == 'Random_Baseline':\n","                    continue\n","                    \n","                if signals[i] != 0:\n","                    # Calculate confidence based on signal consistency\n","                    window_start = max(0, i - 10)\n","                    recent_signals = signals[window_start:i+1]\n","                    \n","                    # Confidence = consistency of recent signals\n","                    non_zero_signals = [s for s in recent_signals if s != 0]\n","                    if len(non_zero_signals) > 0:\n","                        same_direction = sum(1 for s in non_zero_signals if np.sign(s) == np.sign(signals[i]))\n","                        confidence = same_direction / len(non_zero_signals)\n","                    else:\n","                        confidence = 0.5\n","                    \n","                    votes.append(signals[i])\n","                    confidences.append(confidence)\n","            \n","            if votes:\n","                # Weight votes by confidence\n","                weighted_vote = sum(v * c for v, c in zip(votes, confidences))\n","                total_confidence = sum(confidences)\n","                \n","                if total_confidence > 0:\n","                    avg_vote = weighted_vote / total_confidence\n","                    if avg_vote > 0.4:\n","                        combined_signals[i] = 1\n","                    elif avg_vote < -0.4:\n","                        combined_signals[i] = -1\n","        \n","        self.combinations['Confidence_Weighted_Ensemble'] = combined_signals\n","        return combined_signals\n","    \n","    def diversified_portfolio_approach(self):\n","        \"\"\"\n","        Diversified Portfolio Approach\n","        Combine uncorrelated signals to reduce overall strategy risk\n","        \"\"\"\n","        # Group signals by category\n","        signal_categories = {\n","            'momentum': ['Fractal_Hurst', 'KAMA', 'Donchian_Breakout', 'ML_GradientBoosting'],\n","            'mean_reversion': ['VWAP_ZScore', 'Gap_Fill', 'Round_Number_Psychology', 'Vol_Spike_Fade'],\n","            'volume': ['CVD', 'VPT', 'OI_Surge'],\n","            'seasonality': ['Day_of_Week', 'Funding_Reset_Fade', 'Month_End_Flow', 'Weekend_Effect'],\n","            'behavioral': ['Momentum_Exhaustion', 'Round_Number_Psychology']\n","        }\n","        \n","        # Calculate category signals\n","        category_signals = {}\n","        for category, signal_names in signal_categories.items():\n","            category_votes = []\n","            for signal_name in signal_names:\n","                if signal_name in self.signals:\n","                    category_votes.append(self.signals[signal_name])\n","            \n","            if category_votes:\n","                # Average signals within category\n","                category_signal = np.mean(category_votes, axis=0)\n","                category_signals[category] = np.sign(category_signal)\n","        \n","        # Combine categories with equal weight\n","        combined_signals = np.zeros(len(self.data))\n","        \n","        for i in range(len(self.data)):\n","            votes = [cat_sig[i] for cat_sig in category_signals.values() if cat_sig[i] != 0]\n","            \n","            if len(votes) >= 2:  # Require at least 2 categories to agree\n","                avg_vote = np.mean(votes)\n","                if avg_vote > 0.3:\n","                    combined_signals[i] = 1\n","                elif avg_vote < -0.3:\n","                    combined_signals[i] = -1\n","        \n","        self.combinations['Diversified_Portfolio'] = combined_signals\n","        return combined_signals\n","    \n","    def machine_learning_ensemble(self):\n","        \"\"\"\n","        Machine Learning Ensemble\n","        Use ML to learn optimal signal combinations\n","        \"\"\"\n","        try:\n","            from sklearn.ensemble import RandomForestClassifier\n","            from sklearn.model_selection import train_test_split\n","        except ImportError:\n","            print(\"Warning: Scikit-learn not available for ML ensemble\")\n","            return np.zeros(len(self.data))\n","        \n","        # Prepare features (all signals as features)\n","        feature_matrix = []\n","        feature_names = []\n","        \n","        for signal_name, signals in self.signals.items():\n","            if signal_name != 'Random_Baseline':\n","                feature_matrix.append(signals)\n","                feature_names.append(signal_name)\n","        \n","        if len(feature_matrix) < 3:\n","            return np.zeros(len(self.data))\n","        \n","        X = np.array(feature_matrix).T  # Transpose to get samples x features\n","        \n","        # Target: next period return direction\n","        returns = self.data['close'].pct_change().shift(-1)\n","        y = (returns > 0).astype(int)\n","        \n","        # Remove NaN values\n","        valid_indices = ~np.isnan(returns)\n","        X = X[valid_indices]\n","        y = y[valid_indices]\n","        \n","        if len(X) < 100:\n","            return np.zeros(len(self.data))\n","        \n","        # Train-test split\n","        split_idx = int(len(X) * 0.7)\n","        X_train, X_test = X[:split_idx], X[split_idx:]\n","        y_train, y_test = y[:split_idx], y[split_idx:]\n","        \n","        # Train Random Forest\n","        rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n","        rf.fit(X_train, y_train)\n","        \n","        # Generate predictions\n","        predictions = rf.predict_proba(X_test)[:, 1]  # Probability of up move\n","        \n","        # Convert to signals\n","        combined_signals = np.zeros(len(self.data))\n","        test_start_idx = split_idx\n","        \n","        for i, prob in enumerate(predictions):\n","            if test_start_idx + i < len(combined_signals):\n","                if prob > 0.6:\n","                    combined_signals[test_start_idx + i] = 1\n","                elif prob < 0.4:\n","                    combined_signals[test_start_idx + i] = -1\n","        \n","        self.combinations['ML_Ensemble'] = combined_signals\n","        return combined_signals\n","\n","print(\"Ensemble & Weighted Voting Systems implemented\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Hierarchical & Filter-Based Combinations\n","\n","Primary signals with secondary filters and hierarchical decision trees.\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class HierarchicalStrategies:\n","    \"\"\"\n","    Hierarchical & Filter-Based Combinations\n","    Core Concept: Use primary signals with secondary filters for enhanced precision\n","    \"\"\"\n","    \n","    def __init__(self, signals_dict, data):\n","        self.signals = signals_dict\n","        self.data = data\n","        self.combinations = {}\n","    \n","    def momentum_with_volatility_filter(self):\n","        \"\"\"\n","        Momentum Strategy with Volatility Filter\n","        Primary: Momentum signals (KAMA, Fractal_Hurst)\n","        Filter: Only trade in appropriate volatility regime\n","        \"\"\"\n","        # Primary momentum signals\n","        momentum_signals = []\n","        for signal_name in ['KAMA', 'Fractal_Hurst', 'Donchian_Breakout']:\n","            if signal_name in self.signals:\n","                momentum_signals.append(self.signals[signal_name])\n","        \n","        if not momentum_signals:\n","            return np.zeros(len(self.data))\n","        \n","        # Volatility filter\n","        returns = self.data['close'].pct_change()\n","        vol_20d = returns.rolling(20).std() * np.sqrt(252)\n","        vol_percentile = vol_20d.rolling(100).rank(pct=True)\n","        \n","        # Trade momentum only in medium-high volatility (40th-80th percentile)\n","        vol_filter = (vol_percentile > 0.4) & (vol_percentile < 0.8)\n","        \n","        # Combine momentum signals\n","        momentum_consensus = np.mean(momentum_signals, axis=0)\n","        \n","        # Apply filter\n","        combined_signals = np.where(\n","            vol_filter & (momentum_consensus > 0.3), 1,\n","            np.where(vol_filter & (momentum_consensus < -0.3), -1, 0)\n","        )\n","        \n","        self.combinations['Momentum_Vol_Filter'] = combined_signals\n","        return combined_signals\n","    \n","    def mean_reversion_with_trend_filter(self):\n","        \"\"\"\n","        Mean Reversion with Trend Filter\n","        Primary: Mean reversion signals\n","        Filter: Only trade against weak trends, not strong ones\n","        \"\"\"\n","        # Primary mean reversion signals\n","        mean_rev_signals = []\n","        for signal_name in ['VWAP_ZScore', 'Gap_Fill', 'Vol_Spike_Fade']:\n","            if signal_name in self.signals:\n","                mean_rev_signals.append(self.signals[signal_name])\n","        \n","        if not mean_rev_signals:\n","            return np.zeros(len(self.data))\n","        \n","        # Trend strength filter (using moving averages)\n","        sma_20 = self.data['close'].rolling(20).mean()\n","        sma_50 = self.data['close'].rolling(50).mean()\n","        \n","        # Weak trend: MAs are close together\n","        ma_spread = abs(sma_20 - sma_50) / sma_50\n","        weak_trend = ma_spread < 0.05  # Less than 5% spread\n","        \n","        # Combine mean reversion signals\n","        mean_rev_consensus = np.mean(mean_rev_signals, axis=0)\n","        \n","        # Apply filter - only trade mean reversion in weak trends\n","        combined_signals = np.where(\n","            weak_trend & (mean_rev_consensus > 0.3), 1,\n","            np.where(weak_trend & (mean_rev_consensus < -0.3), -1, 0)\n","        )\n","        \n","        self.combinations['MeanRev_Trend_Filter'] = combined_signals\n","        return combined_signals\n","    \n","    def breakout_with_volume_confirmation(self):\n","        \"\"\"\n","        Breakout Strategy with Volume Confirmation\n","        Primary: Breakout signals\n","        Filter: Require volume confirmation\n","        \"\"\"\n","        # Primary breakout signals\n","        breakout_signals = []\n","        for signal_name in ['Donchian_Breakout', 'BB_Squeeze', 'Range_Compression']:\n","            if signal_name in self.signals:\n","                breakout_signals.append(self.signals[signal_name])\n","        \n","        if not breakout_signals or 'volume' not in self.data.columns:\n","            return np.zeros(len(self.data))\n","        \n","        # Volume confirmation filter\n","        vol_ma = self.data['volume'].rolling(20).mean()\n","        high_volume = self.data['volume'] > vol_ma * 1.3\n","        \n","        # Combine breakout signals\n","        breakout_consensus = np.mean(breakout_signals, axis=0)\n","        \n","        # Apply volume filter\n","        combined_signals = np.where(\n","            high_volume & (breakout_consensus > 0.3), 1,\n","            np.where(high_volume & (breakout_consensus < -0.3), -1, 0)\n","        )\n","        \n","        self.combinations['Breakout_Volume_Filter'] = combined_signals\n","        return combined_signals\n","    \n","    def ml_primary_with_seasonal_filter(self):\n","        \"\"\"\n","        ML Primary with Seasonal Filter\n","        Primary: ML signals\n","        Filter: Seasonal timing filters\n","        \"\"\"\n","        # Primary ML signals\n","        ml_signals = []\n","        for signal_name in ['ML_GradientBoosting', 'ML_RegimeClassifier']:\n","            if signal_name in self.signals:\n","                ml_signals.append(self.signals[signal_name])\n","        \n","        if not ml_signals:\n","            return np.zeros(len(self.data))\n","        \n","        # Seasonal filters\n","        seasonal_signals = []\n","        for signal_name in ['Day_of_Week', 'Month_End_Flow', 'Weekend_Effect']:\n","            if signal_name in self.signals:\n","                seasonal_signals.append(self.signals[signal_name])\n","        \n","        # Combine ML signals\n","        ml_consensus = np.mean(ml_signals, axis=0)\n","        \n","        # Apply seasonal filter\n","        if seasonal_signals:\n","            seasonal_consensus = np.mean(seasonal_signals, axis=0)\n","            # Only trade when seasonal signals don't contradict\n","            seasonal_neutral = abs(seasonal_consensus) < 0.3\n","            \n","            combined_signals = np.where(\n","                seasonal_neutral & (ml_consensus > 0.4), 1,\n","                np.where(seasonal_neutral & (ml_consensus < -0.4), -1, 0)\n","            )\n","        else:\n","            combined_signals = np.where(\n","                ml_consensus > 0.4, 1,\n","                np.where(ml_consensus < -0.4, -1, 0)\n","            )\n","        \n","        self.combinations['ML_Seasonal_Filter'] = combined_signals\n","        return combined_signals\n","    \n","    def hierarchical_decision_tree(self):\n","        \"\"\"\n","        Hierarchical Decision Tree\n","        Multi-level decision process with different signal priorities\n","        \"\"\"\n","        combined_signals = np.zeros(len(self.data))\n","        \n","        for i in range(len(self.data)):\n","            # Level 1: Check regime signals first\n","            regime_vote = 0\n","            regime_count = 0\n","            \n","            for signal_name in ['RV_Regime', 'ML_RegimeClassifier']:\n","                if signal_name in self.signals and self.signals[signal_name][i] != 0:\n","                    regime_vote += self.signals[signal_name][i]\n","                    regime_count += 1\n","            \n","            if regime_count > 0:\n","                regime_consensus = regime_vote / regime_count\n","                \n","                # Level 2: Based on regime, choose appropriate signals\n","                if regime_consensus > 0.3:  # Bullish regime\n","                    # Use momentum signals\n","                    momentum_vote = 0\n","                    momentum_count = 0\n","                    \n","                    for signal_name in ['KAMA', 'Donchian_Breakout', 'Fractal_Hurst']:\n","                        if signal_name in self.signals and self.signals[signal_name][i] != 0:\n","                            momentum_vote += self.signals[signal_name][i]\n","                            momentum_count += 1\n","                    \n","                    if momentum_count >= 2:  # Require at least 2 momentum signals\n","                        momentum_consensus = momentum_vote / momentum_count\n","                        if momentum_consensus > 0.5:\n","                            combined_signals[i] = 1\n","                \n","                elif regime_consensus < -0.3:  # Bearish regime\n","                    # Use defensive/contrarian signals\n","                    defensive_vote = 0\n","                    defensive_count = 0\n","                    \n","                    for signal_name in ['VWAP_ZScore', 'Gap_Fill', 'Vol_Spike_Fade']:\n","                        if signal_name in self.signals and self.signals[signal_name][i] != 0:\n","                            defensive_vote += self.signals[signal_name][i]\n","                            defensive_count += 1\n","                    \n","                    if defensive_count >= 2:\n","                        defensive_consensus = defensive_vote / defensive_count\n","                        if defensive_consensus < -0.5:\n","                            combined_signals[i] = -1\n","                        elif defensive_consensus > 0.5:  # Counter-trend opportunity\n","                            combined_signals[i] = 1\n","                \n","                # Level 3: Apply final filters\n","                # Volume filter\n","                if 'volume' in self.data.columns:\n","                    vol_ma = self.data['volume'].rolling(20).mean()\n","                    if i >= 20 and self.data['volume'].iloc[i] < vol_ma.iloc[i] * 0.7:\n","                        combined_signals[i] = 0  # Cancel signal on very low volume\n","        \n","        self.combinations['Hierarchical_Decision_Tree'] = combined_signals\n","        return combined_signals\n","\n","print(\"Hierarchical & Filter-Based Combinations implemented\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Comprehensive Combination Testing Framework\n","\n","Test all combination strategies and rank their performance.\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def run_all_combination_strategies(signals_dict, data):\n","    \"\"\"\n","    Run all combination strategies and return comprehensive results\n","    \"\"\"\n","    print(\"Running ALL Signal Combination Strategies...\")\n","    print(f\"Available signals: {list(signals_dict.keys())}\")\n","    \n","    all_combinations = {}\n","    \n","    # Initialize all combination classes\n","    regime_aware = RegimeAwareCombinations(signals_dict, data)\n","    confirmation = ConfirmationStrategies(signals_dict, data)\n","    ensemble = EnsembleStrategies(signals_dict, data)\n","    hierarchical = HierarchicalStrategies(signals_dict, data)\n","    \n","    print(\"\\nCalculating Regime-Aware Combinations...\")\n","    try:\n","        vol_regime = regime_aware.volatility_regime_strategy()\n","        all_combinations['Volatility_Regime'] = vol_regime\n","        print(\"Volatility Regime Strategy - Done\")\n","    except Exception as e:\n","        print(f\"Volatility Regime failed: {e}\")\n","    \n","    # Execute all strategies with simplified logging\n","    strategies = [\n","        ('trend_vs_chop_strategy', 'Trend_vs_Chop'),\n","        ('volume_regime_strategy', 'Volume_Regime'),\n","    ]\n","    \n","    for method_name, combo_name in strategies:\n","        try:\n","            method = getattr(regime_aware, method_name)\n","            all_combinations[combo_name] = method()\n","            print(f\"{combo_name} - Done\")\n","        except Exception as e:\n","            print(f\"{combo_name} failed: {e}\")\n","    \n","    # Confirmation strategies\n","    confirmation_strategies = [\n","        ('momentum_confirmation_strategy', 'Momentum_Confirmation'),\n","        ('mean_reversion_confirmation_strategy', 'Mean_Reversion_Confirmation'),\n","        ('cross_timeframe_confirmation', 'Cross_Timeframe_Confirmation'),\n","        ('volume_price_confirmation', 'Volume_Price_Confirmation'),\n","    ]\n","    \n","    for method_name, combo_name in confirmation_strategies:\n","        try:\n","            method = getattr(confirmation, method_name)\n","            all_combinations[combo_name] = method()\n","            print(f\"{combo_name} - Done\")\n","        except Exception as e:\n","            print(f\"{combo_name} failed: {e}\")\n","    \n","    # Ensemble strategies\n","    ensemble_strategies = [\n","        ('adaptive_weighted_ensemble', 'Adaptive_Weighted_Ensemble'),\n","        ('confidence_weighted_ensemble', 'Confidence_Weighted_Ensemble'),\n","        ('diversified_portfolio_approach', 'Diversified_Portfolio'),\n","        ('machine_learning_ensemble', 'ML_Ensemble'),\n","    ]\n","    \n","    for method_name, combo_name in ensemble_strategies:\n","        try:\n","            method = getattr(ensemble, method_name)\n","            all_combinations[combo_name] = method()\n","            print(f\"{combo_name} - Done\")\n","        except Exception as e:\n","            print(f\"{combo_name} failed: {e}\")\n","    \n","    # Hierarchical strategies\n","    hierarchical_strategies = [\n","        ('momentum_with_volatility_filter', 'Momentum_Vol_Filter'),\n","        ('mean_reversion_with_trend_filter', 'MeanRev_Trend_Filter'),\n","        ('breakout_with_volume_confirmation', 'Breakout_Volume_Filter'),\n","        ('ml_primary_with_seasonal_filter', 'ML_Seasonal_Filter'),\n","        ('hierarchical_decision_tree', 'Hierarchical_Decision_Tree'),\n","    ]\n","    \n","    for method_name, combo_name in hierarchical_strategies:\n","        try:\n","            method = getattr(hierarchical, method_name)\n","            all_combinations[combo_name] = method()\n","            print(f\"{combo_name} - Done\")\n","        except Exception as e:\n","            print(f\"{combo_name} failed: {e}\")\n","    \n","    # Add random baseline\n","    np.random.seed(42)\n","    random_signals = np.random.choice([-1, 0, 1], size=len(data), p=[0.3, 0.4, 0.3])\n","    all_combinations['Random_Baseline'] = random_signals\n","    print(\"Random Baseline added\")\n","    \n","    print(f\"\\nSuccessfully calculated {len(all_combinations)} combination strategies!\")\n","    return all_combinations\n","\n","# Performance analysis for combinations\n","class CombinationPerformanceAnalyzer:\n","    \"\"\"\n","    Analyze performance of all combination strategies\n","    \"\"\"\n","    \n","    def __init__(self):\n","        self.results = {}\n","    \n","    def analyze_combination_performance(self, data, signals, strategy_name):\n","        \"\"\"\n","        Analyze performance of a combination strategy\n","        \"\"\"\n","        # Calculate forward returns\n","        forward_returns = data['close'].pct_change().shift(-1)\n","        \n","        # Signal performance metrics\n","        long_signals = signals == 1\n","        short_signals = signals == -1\n","        \n","        if np.sum(long_signals) > 0:\n","            long_returns = forward_returns[long_signals]\n","            avg_long_return = long_returns.mean()\n","            long_win_rate = (long_returns > 0).mean()\n","        else:\n","            avg_long_return = 0\n","            long_win_rate = 0\n","        \n","        if np.sum(short_signals) > 0:\n","            short_returns = -forward_returns[short_signals]\n","            avg_short_return = short_returns.mean()\n","            short_win_rate = (short_returns > 0).mean()\n","        else:\n","            avg_short_return = 0\n","            short_win_rate = 0\n","        \n","        # Overall metrics\n","        signal_returns = np.where(signals == 1, forward_returns,\n","                                 np.where(signals == -1, -forward_returns, 0))\n","        \n","        total_return = np.sum(signal_returns)\n","        avg_return = np.mean(signal_returns[signals != 0]) if np.sum(signals != 0) > 0 else 0\n","        volatility = np.std(signal_returns[signals != 0]) if np.sum(signals != 0) > 0 else 0\n","        sharpe_ratio = avg_return / volatility if volatility > 0 else 0\n","        \n","        # Signal frequency\n","        signal_frequency = np.sum(signals != 0) / len(signals)\n","        \n","        # Calmar ratio (return / max drawdown)\n","        cumulative_returns = np.cumsum(signal_returns)\n","        running_max = np.maximum.accumulate(cumulative_returns)\n","        drawdown = running_max - cumulative_returns\n","        max_drawdown = np.max(drawdown) if len(drawdown) > 0 else 0\n","        calmar_ratio = total_return / max_drawdown if max_drawdown > 0 else 0\n","        \n","        return {\n","            'strategy_name': strategy_name,\n","            'total_return': total_return,\n","            'avg_return': avg_return,\n","            'volatility': volatility,\n","            'sharpe_ratio': sharpe_ratio,\n","            'calmar_ratio': calmar_ratio,\n","            'max_drawdown': max_drawdown,\n","            'signal_frequency': signal_frequency,\n","            'long_return': avg_long_return,\n","            'short_return': avg_short_return,\n","            'long_win_rate': long_win_rate,\n","            'short_win_rate': short_win_rate,\n","            'total_signals': np.sum(signals != 0)\n","        }\n","    \n","    def run_comprehensive_combination_analysis(self, data, all_combinations):\n","        \"\"\"\n","        Run comprehensive analysis on all combination strategies\n","        \"\"\"\n","        all_performance = []\n","        \n","        print(f\"\\nAnalyzing {len(all_combinations)} combination strategies...\")\n","        \n","        for strategy_name, signals in all_combinations.items():\n","            try:\n","                performance = self.analyze_combination_performance(data, signals, strategy_name)\n","                all_performance.append(performance)\n","                print(f\"{strategy_name}: Sharpe {performance['sharpe_ratio']:.3f}\")\n","            except Exception as e:\n","                print(f\"{strategy_name} analysis failed: {e}\")\n","        \n","        # Create performance DataFrame\n","        performance_df = pd.DataFrame(all_performance)\n","        return performance_df.sort_values('sharpe_ratio', ascending=False)\n","\n","print(\"Comprehensive Combination Testing Framework implemented\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Summary & Recommended Combinations\n","\n","### Top Signal Combination Strategies\n","\n","Based on theoretical foundations, here are the most promising combinations:\n","\n","#### Tier 1: High-Confidence Combinations\n","\n","1. **Volatility Regime Strategy**\n","   - Logic: Use momentum signals in high-vol periods, mean-reversion in low-vol\n","   - Edge: Adapts to market conditions automatically\n","\n","2. **Momentum Confirmation Strategy**\n","   - Logic: Require 70% agreement from 3+ momentum signals\n","   - Edge: Reduces false positives through consensus\n","\n","3. **Adaptive Weighted Ensemble**\n","   - Logic: Weight signals by recent Sharpe ratio performance\n","   - Edge: Self-improving system that adapts to changing conditions\n","\n","#### Tier 2: Robust Combinations\n","\n","4. **Hierarchical Decision Tree**\n","   - Logic: Multi-level decision process with regime detection first\n","\n","5. **Cross-Timeframe Confirmation**\n","   - Logic: Fast signals confirmed by slow signals\n","\n","6. **Volume-Price Confirmation**\n","   - Logic: Price signals must be confirmed by volume\n","\n","### Implementation Priority\n","\n","1. Start with: Volatility Regime Strategy\n","2. Add next: Momentum Confirmation Strategy\n","3. Advanced: Adaptive Weighted Ensemble\n","\n","### Key Implementation Notes\n","\n","1. Ensure OHLCV data with sufficient history\n","2. Test regime detection accuracy\n","3. Account for calculation delays in live trading\n","4. Factor in realistic transaction costs\n","5. Implement position sizing and stop-losses\n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Execute All Combinations & Show Results\n","\n","Run the comprehensive analysis and display performance rankings.\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# First, let's create some sample data and signals for demonstration\n","# In practice, you would import the signals from the main signals notebook\n","\n","# Sample BTC data setup\n","btc_data = qb.add_crypto(\"BTCUSD\", market=Market.KRAKEN)\n","end_date = datetime.now()\n","start_date = end_date - timedelta(days=365)\n","\n","print(\"Fetching BTC data for combinations analysis...\")\n","history = qb.history([btc_data.symbol], start_date, end_date, Resolution.DAILY)\n","\n","if not history.empty:\n","    btc_df = history.reset_index()\n","    btc_df = btc_df.set_index('time')[['open', 'high', 'low', 'close', 'volume']]\n","    print(f\"Data loaded: {len(btc_df)} data points\")\n","    \n","    # Create sample signals for demonstration\n","    # In practice, import these from your main signals analysis\n","    sample_signals = {\n","        'Fractal_Hurst': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'KAMA': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.25, 0.5, 0.25]),\n","        'Donchian_Breakout': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.15, 0.7, 0.15]),\n","        'VWAP_ZScore': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'BB_Squeeze': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'RV_Regime': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.3, 0.4, 0.3]),\n","        'ML_GradientBoosting': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'CVD': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.25, 0.5, 0.25]),\n","        'VPT': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'Day_of_Week': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.15, 0.7, 0.15]),\n","        'Gap_Fill': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Vol_Spike_Fade': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.15, 0.7, 0.15]),\n","        'Round_Number_Psychology': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Range_Compression': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'OI_Surge': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.05, 0.9, 0.05]),\n","        'ML_RegimeClassifier': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.3, 0.4, 0.3]),\n","        'Funding_Reset_Fade': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Month_End_Flow': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.05, 0.9, 0.05]),\n","        'Weekend_Effect': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Momentum_Exhaustion': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1])\n","    }\n","    \n","    print(f\"Sample signals created: {len(sample_signals)} signals\")\n","    print(\"Note: Using sample signals for demonstration. In practice, import from main signals analysis.\")\n","    \n","else:\n","    print(\"No data available. Using synthetic data for demonstration.\")\n","    # Create synthetic data\n","    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n","    np.random.seed(42)\n","    \n","    # Generate realistic BTC price data\n","    initial_price = 50000\n","    returns = np.random.normal(0.001, 0.03, len(dates))  # Daily returns\n","    prices = [initial_price]\n","    \n","    for ret in returns[1:]:\n","        prices.append(prices[-1] * (1 + ret))\n","    \n","    btc_df = pd.DataFrame({\n","        'open': prices,\n","        'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],\n","        'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],\n","        'close': prices,\n","        'volume': np.random.lognormal(10, 0.5, len(dates))\n","    }, index=dates)\n","    \n","    # Create sample signals\n","    sample_signals = {\n","        'Fractal_Hurst': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'KAMA': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.25, 0.5, 0.25]),\n","        'Donchian_Breakout': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.15, 0.7, 0.15]),\n","        'VWAP_ZScore': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'BB_Squeeze': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'RV_Regime': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.3, 0.4, 0.3]),\n","        'ML_GradientBoosting': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'CVD': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.25, 0.5, 0.25]),\n","        'VPT': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.2, 0.6, 0.2]),\n","        'Day_of_Week': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.15, 0.7, 0.15]),\n","        'Gap_Fill': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Vol_Spike_Fade': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.15, 0.7, 0.15]),\n","        'Round_Number_Psychology': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Range_Compression': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'OI_Surge': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.05, 0.9, 0.05]),\n","        'ML_RegimeClassifier': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.3, 0.4, 0.3]),\n","        'Funding_Reset_Fade': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Month_End_Flow': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.05, 0.9, 0.05]),\n","        'Weekend_Effect': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1]),\n","        'Momentum_Exhaustion': np.random.choice([-1, 0, 1], size=len(btc_df), p=[0.1, 0.8, 0.1])\n","    }\n","    \n","    print(f\"Synthetic data created: {len(btc_df)} data points\")\n","    print(f\"Sample signals created: {len(sample_signals)} signals\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Run all combination strategies\n","print(\"EXECUTING ALL SIGNAL COMBINATIONS\")\n","print(\"=\" * 60)\n","\n","# Execute all combinations\n","all_combinations = run_all_combination_strategies(sample_signals, btc_df)\n","\n","print(f\"\\nCOMBINATION STRATEGIES SUMMARY\")\n","print(f\"Total combinations created: {len(all_combinations)}\")\n","print(f\"Data period: {btc_df.index[0].strftime('%Y-%m-%d')} to {btc_df.index[-1].strftime('%Y-%m-%d')}\")\n","print(f\"Total data points: {len(btc_df)}\")\n","\n","# Show signal frequency for each combination\n","print(f\"\\nSIGNAL FREQUENCY ANALYSIS\")\n","print(\"-\" * 40)\n","for combo_name, signals in all_combinations.items():\n","    total_signals = np.sum(signals != 0)\n","    signal_frequency = total_signals / len(signals)\n","    long_signals = np.sum(signals == 1)\n","    short_signals = np.sum(signals == -1)\n","    \n","    print(f\"{combo_name:<30}: {signal_frequency:>6.1%} ({long_signals:>3} long, {short_signals:>3} short)\")\n","\n","print(\"\\n\" + \"=\" * 60)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Run comprehensive performance analysis\n","print(\"COMPREHENSIVE PERFORMANCE ANALYSIS\")\n","print(\"=\" * 60)\n","\n","analyzer = CombinationPerformanceAnalyzer()\n","performance_results = analyzer.run_comprehensive_combination_analysis(btc_df, all_combinations)\n","\n","print(f\"\\nTOP PERFORMING COMBINATION STRATEGIES\")\n","print(\"=\" * 80)\n","\n","# Display top 10 strategies\n","top_strategies = performance_results.head(10)\n","\n","print(f\"{'Rank':<4} {'Strategy':<30} {'Sharpe':<8} {'Calmar':<8} {'Total Ret':<10} {'Frequency':<10} {'Signals':<8}\")\n","print(\"-\" * 80)\n","\n","for i, (_, row) in enumerate(top_strategies.iterrows(), 1):\n","    print(f\"{i:<4} {row['strategy_name']:<30} {row['sharpe_ratio']:<8.3f} {row['calmar_ratio']:<8.3f} \"\n","          f\"{row['total_return']:<10.3f} {row['signal_frequency']:<10.2%} {row['total_signals']:<8.0f}\")\n","\n","# Performance statistics\n","print(f\"\\nPERFORMANCE STATISTICS\")\n","print(\"-\" * 40)\n","\n","# Best performers\n","best_sharpe = performance_results.iloc[0]\n","best_calmar = performance_results.loc[performance_results['calmar_ratio'].idxmax()]\n","best_return = performance_results.loc[performance_results['total_return'].idxmax()]\n","\n","print(f\"Best Sharpe Ratio: {best_sharpe['strategy_name']} ({best_sharpe['sharpe_ratio']:.3f})\")\n","print(f\"Best Calmar Ratio: {best_calmar['strategy_name']} ({best_calmar['calmar_ratio']:.3f})\")\n","print(f\"Best Total Return: {best_return['strategy_name']} ({best_return['total_return']:.3f})\")\n","\n","# Random baseline comparison\n","random_performance = performance_results[performance_results['strategy_name'] == 'Random_Baseline']\n","if not random_performance.empty:\n","    random_sharpe = random_performance.iloc[0]['sharpe_ratio']\n","    random_return = random_performance.iloc[0]['total_return']\n","    \n","    print(f\"\\nRandom Baseline Performance:\")\n","    print(f\"   Sharpe Ratio: {random_sharpe:.3f}\")\n","    print(f\"   Total Return: {random_return:.3f}\")\n","    \n","    # Count strategies beating random\n","    better_sharpe = performance_results[performance_results['sharpe_ratio'] > random_sharpe]\n","    better_return = performance_results[performance_results['total_return'] > random_return]\n","    \n","    print(f\"\\nStrategies beating random baseline:\")\n","    print(f\"   By Sharpe Ratio: {len(better_sharpe)}/{len(performance_results)-1}\")\n","    print(f\"   By Total Return: {len(better_return)}/{len(performance_results)-1}\")\n","    \n","    if len(better_sharpe) > 1:  # Exclude random itself\n","        print(f\"\\nTop strategies beating random (Sharpe):\")\n","        for _, row in better_sharpe.head(5).iterrows():\n","            if row['strategy_name'] != 'Random_Baseline':\n","                print(f\"   - {row['strategy_name']}: {row['sharpe_ratio']:.3f}\")\n","\n","print(\"\\n\" + \"=\" * 80)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Detailed analysis by combination category\n","print(\"DETAILED ANALYSIS BY CATEGORY\")\n","print(\"=\" * 60)\n","\n","# Categorize strategies\n","categories = {\n","    'Regime-Aware': ['Volatility_Regime', 'Trend_vs_Chop', 'Volume_Regime'],\n","    'Confirmation-Based': ['Momentum_Confirmation', 'Mean_Reversion_Confirmation', \n","                          'Cross_Timeframe_Confirmation', 'Volume_Price_Confirmation'],\n","    'Ensemble Methods': ['Adaptive_Weighted_Ensemble', 'Confidence_Weighted_Ensemble', \n","                        'Diversified_Portfolio', 'ML_Ensemble'],\n","    'Hierarchical/Filtered': ['Momentum_Vol_Filter', 'MeanRev_Trend_Filter', \n","                             'Breakout_Volume_Filter', 'ML_Seasonal_Filter', \n","                             'Hierarchical_Decision_Tree']\n","}\n","\n","for category, strategy_names in categories.items():\n","    print(f\"\\n{category.upper()}\")\n","    print(\"-\" * 40)\n","    \n","    category_results = performance_results[performance_results['strategy_name'].isin(strategy_names)]\n","    \n","    if not category_results.empty:\n","        # Sort by Sharpe ratio\n","        category_results = category_results.sort_values('sharpe_ratio', ascending=False)\n","        \n","        print(f\"{'Strategy':<30} {'Sharpe':<8} {'Return':<8} {'Frequency':<10}\")\n","        print(\"-\" * 60)\n","        \n","        for _, row in category_results.iterrows():\n","            print(f\"{row['strategy_name']:<30} {row['sharpe_ratio']:<8.3f} \"\n","                  f\"{row['total_return']:<8.3f} {row['signal_frequency']:<10.2%}\")\n","        \n","        # Category statistics\n","        avg_sharpe = category_results['sharpe_ratio'].mean()\n","        avg_return = category_results['total_return'].mean()\n","        avg_frequency = category_results['signal_frequency'].mean()\n","        \n","        print(f\"\\nCategory Averages: Sharpe {avg_sharpe:.3f}, Return {avg_return:.3f}, Frequency {avg_frequency:.2%}\")\n","    else:\n","        print(\"No strategies found in this category\")\n","\n","print(\"\\n\" + \"=\" * 60)\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Create comprehensive visualization\n","plt.style.use('seaborn-v0_8')\n","fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","\n","# Plot 1: Sharpe Ratio Ranking\n","ax1 = axes[0, 0]\n","top_15 = performance_results.head(15)\n","colors = ['red' if name == 'Random_Baseline' else 'green' if sharpe > 0 else 'red' \n","          for name, sharpe in zip(top_15['strategy_name'], top_15['sharpe_ratio'])]\n","\n","bars1 = ax1.barh(range(len(top_15)), top_15['sharpe_ratio'], color=colors, alpha=0.7)\n","ax1.set_yticks(range(len(top_15)))\n","ax1.set_yticklabels(top_15['strategy_name'], fontsize=8)\n","ax1.set_xlabel('Sharpe Ratio')\n","ax1.set_title('Top 15 Strategies by Sharpe Ratio', fontweight='bold')\n","ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n","ax1.grid(True, alpha=0.3)\n","\n","# Plot 2: Total Return vs Signal Frequency\n","ax2 = axes[0, 1]\n","scatter_colors = ['red' if name == 'Random_Baseline' else 'blue' \n","                 for name in performance_results['strategy_name']]\n","\n","scatter = ax2.scatter(performance_results['signal_frequency'] * 100, \n","                     performance_results['total_return'],\n","                     c=scatter_colors, alpha=0.7, s=60)\n","\n","ax2.set_xlabel('Signal Frequency (%)')\n","ax2.set_ylabel('Total Return')\n","ax2.set_title('Return vs Signal Frequency', fontweight='bold')\n","ax2.grid(True, alpha=0.3)\n","\n","# Add labels for top performers\n","for _, row in performance_results.head(5).iterrows():\n","    ax2.annotate(row['strategy_name'][:15], \n","                (row['signal_frequency'] * 100, row['total_return']),\n","                xytext=(5, 5), textcoords='offset points', fontsize=8)\n","\n","# Plot 3: Category Performance Comparison\n","ax3 = axes[1, 0]\n","category_performance = {}\n","\n","for category, strategy_names in categories.items():\n","    category_results = performance_results[performance_results['strategy_name'].isin(strategy_names)]\n","    if not category_results.empty:\n","        category_performance[category] = category_results['sharpe_ratio'].mean()\n","\n","if category_performance:\n","    categories_list = list(category_performance.keys())\n","    sharpe_values = list(category_performance.values())\n","    \n","    bars3 = ax3.bar(categories_list, sharpe_values, color='skyblue', alpha=0.7)\n","    ax3.set_ylabel('Average Sharpe Ratio')\n","    ax3.set_title('Average Performance by Category', fontweight='bold')\n","    ax3.tick_params(axis='x', rotation=45)\n","    ax3.grid(True, alpha=0.3)\n","\n","# Plot 4: Risk-Return Profile\n","ax4 = axes[1, 1]\n","risk_return_colors = ['red' if name == 'Random_Baseline' else 'green' if sharpe > 0.1 else 'orange'\n","                     for name, sharpe in zip(performance_results['strategy_name'], \n","                                           performance_results['sharpe_ratio'])]\n","\n","ax4.scatter(performance_results['volatility'], \n","           performance_results['total_return'],\n","           c=risk_return_colors, alpha=0.7, s=60)\n","\n","ax4.set_xlabel('Volatility')\n","ax4.set_ylabel('Total Return')\n","ax4.set_title('Risk-Return Profile', fontweight='bold')\n","ax4.grid(True, alpha=0.3)\n","\n","# Add labels for extreme points\n","for _, row in performance_results.head(3).iterrows():\n","    ax4.annotate(row['strategy_name'][:12], \n","                (row['volatility'], row['total_return']),\n","                xytext=(5, 5), textcoords='offset points', fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Comprehensive visualizations generated!\")\n","print(\"\\nKEY INSIGHTS FROM RESULTS:\")\n","print(\"=\" * 50)\n","\n","# Generate insights\n","if not performance_results.empty:\n","    best_strategy = performance_results.iloc[0]\n","    print(f\"Best Overall Strategy: {best_strategy['strategy_name']}\")\n","    print(f\"   - Sharpe Ratio: {best_strategy['sharpe_ratio']:.3f}\")\n","    print(f\"   - Total Return: {best_strategy['total_return']:.3f}\")\n","    print(f\"   - Signal Frequency: {best_strategy['signal_frequency']:.2%}\")\n","    print(f\"   - Total Signals: {best_strategy['total_signals']:.0f}\")\n","    \n","    # Category insights\n","    if category_performance:\n","        best_category = max(category_performance.keys(), key=lambda x: category_performance[x])\n","        print(f\"\\nBest Performing Category: {best_category}\")\n","        print(f\"   - Average Sharpe: {category_performance[best_category]:.3f}\")\n","    \n","    # Signal frequency insights\n","    high_freq_strategies = performance_results[performance_results['signal_frequency'] > 0.3]\n","    low_freq_strategies = performance_results[performance_results['signal_frequency'] < 0.1]\n","    \n","    if not high_freq_strategies.empty:\n","        print(f\"\\nHigh-Frequency Strategies (>30% signals): {len(high_freq_strategies)}\")\n","        print(f\"   - Average Sharpe: {high_freq_strategies['sharpe_ratio'].mean():.3f}\")\n","    \n","    if not low_freq_strategies.empty:\n","        print(f\"\\nLow-Frequency Strategies (<10% signals): {len(low_freq_strategies)}\")\n","        print(f\"   - Average Sharpe: {low_freq_strategies['sharpe_ratio'].mean():.3f}\")\n","\n","print(\"\\n\" + \"=\" * 50)\n","print(\"COMPREHENSIVE SIGNAL COMBINATIONS ANALYSIS COMPLETE!\")\n","print(\"Ready for implementation and live testing!\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# End of notebook\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Foundation-Py-Default","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}