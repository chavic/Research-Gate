{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Playbook\n",
    "\n",
    "Notebook scaffold for ranking assets based on breakout heuristics, on-chain flows, and risk filters.\n",
    "\n",
    "## TODO\n",
    "- [ ] Load price, volume, and on-chain snapshots from QuantConnect and CoinGecko datasets.\n",
    "- [ ] Engineer factor buckets (acceleration, relative volume, whale concentration, social buzz).\n",
    "- [ ] Assemble composite selection scores, tier assets, and export candidate lists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Data Feeds\n",
    "\n",
    "### QuantConnect Native\n",
    "- **Benzinga News / Tiingo News** – headline sentiment and metadata for BTC, ETH, and select large-cap alts; useful for fast event confirmation but thin coverage on long-tail coins.\n",
    "- **Brain Language Metrics** – relevance/novelty scores derived from global news; can flag macro narratives that bleed into crypto momentum.\n",
    "- **Quiver Social Sentiment** – Reddit/Twitter aggregates; picks up mainstream retail chatter around flagship tokens and crypto-linked equities.\n",
    "- **CoinGecko Market Cap (QC dataset)** – market-cap ranks, dominance, turnover stats to support universe sizing and relative-strength filters.\n",
    "\n",
    "### External Integrations (to stitch in)\n",
    "- **Santiment / Whale Alerts** – wallet concentration, whale inflows/outflows, crowd sentiment scores for mid-cap alts.\n",
    "- **LunarCrush / CryptoQuant** – social dominance, exchange reserves, funding metrics across major venues.\n",
    "- **On-chain Indexers (Glassnode, Nansen, Dune queries)** – active addresses, supply dynamics, staking unlock calendars tailored per chain.\n",
    "- **Twitter / Reddit Firehose (via third-party APIs)** – custom NLP pipelines for influencer momentum and community buzz on smaller tokens.\n",
    "\n",
    "### Operational Notes\n",
    "- Align each dataset with `DataRequestSpec` entries for reproducibility.\n",
    "- Track licensing/latency tiers (QuantConnect datasets vs. BYO APIs) before wiring into live selection screens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Playbook\n",
    "\n",
    "### Price & Structure\n",
    "- Donchian and pivot breakouts, ATR/Keltner width squeezes, multi-horizon momentum tiers.\n",
    "- VWAP/anchored VWAP drift, mean-reversion z-scores, regime change detectors.\n",
    "\n",
    "### Volume & Liquidity\n",
    "- Relative volume percentiles, cumulative volume delta, book imbalance and depth elasticity.\n",
    "- Venue rotation and stablecoin inflow measures for cross-exchange appetite.\n",
    "\n",
    "### On-chain Dynamics\n",
    "- Whale wallet inflows/outflows, active addresses growth, NVT/NVM deviations.\n",
    "- Exchange reserve delta, staking unlock calendars, supply concentration shifts.\n",
    "\n",
    "### Sentiment & Behavior\n",
    "- Headline polarity, social volume acceleration, influencer-trigger alerts.\n",
    "- Reddit/Twitter NLP scores, Google Trends spikes, crowd fear/greed gauges.\n",
    "\n",
    "### Derivatives & Funding\n",
    "- Perp funding rate momentum, basis term-structure kinks, liquidation heatmaps.\n",
    "- Options IV rank/skew, realized–implied spread alerts.\n",
    "\n",
    "### Macro & Network Overlays\n",
    "- Crypto beta vs. DXY/real yields, global liquidity proxies, volatility regime classification.\n",
    "- Metcalfe-style network valuation residuals, hash-rate versus price divergence checks.\n",
    "\n",
    "## Combo Ideas\n",
    "- Breakout confirmation: squeeze + 20/60d momentum + relative volume + whale inflow filter.\n",
    "- Sentiment spike: NLP delta + social dominance + exchange outflows with liquidity guardrails.\n",
    "- Whale tracking: large transactions + OBV slope + funding normalization to avoid blow-offs.\n",
    "- Network value overlay: momentum stack gated by NVT/NVM and active address acceleration.\n",
    "- Volatility expansion: realized–implied spread + liquidity expansion + network residual drift.\n",
    "- Macro regime: crypto momentum adjusted by real-rate residuals and stablecoin supply growth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet quantconnect-stubs\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:  # pragma: no cover - fallback for non-notebook contexts\n",
    "    display = None\n",
    "\n",
    "from research.scripts.data_loader import DataLoader, DataRequestSpec\n",
    "\n",
    "pd.options.display.max_columns = 20\n",
    "\n",
    "try:\n",
    "    from QuantConnect import Resolution  # type: ignore[import]\n",
    "    from QuantConnect.Research import QuantBook  # type: ignore[import]\n",
    "except ImportError as exc:  # pragma: no cover - requires QuantConnect runtime\n",
    "    raise RuntimeError(\n",
    "        \"QuantConnect Research assemblies not found. Run inside a Lean Research environment.\"\n",
    "    ) from exc\n",
    "\n",
    "try:\n",
    "    plt.style.use(\"seaborn-v0_8\")\n",
    "except OSError:  # pragma: no cover - fallback if style unavailable\n",
    "    plt.style.use(\"seaborn\")\n",
    "\n",
    "qb = QuantBook()\n",
    "\n",
    "\n",
    "def enum_to_str(enum_obj) -> str:\n",
    "    \"\"\"Return a string representation of a QuantConnect enum.\"\"\"\n",
    "\n",
    "    if hasattr(enum_obj, \"ToString\"):\n",
    "        return enum_obj.ToString()\n",
    "    name = getattr(enum_obj, \"name\", None)\n",
    "    if name is not None:\n",
    "        return name\n",
    "    return str(enum_obj)\n",
    "\n",
    "\n",
    "def add_crypto_universe(tickers: Iterable[str], resolution: Resolution = Resolution.Hour):\n",
    "    \"\"\"Register crypto assets for the research universe and return their Symbol objects.\"\"\"\n",
    "\n",
    "    symbols = []\n",
    "    for ticker in tickers:\n",
    "        symbol = qb.AddCrypto(ticker, resolution).Symbol\n",
    "        symbols.append(symbol)\n",
    "    return symbols\n",
    "\n",
    "\n",
    "UNIVERSE = [\"BTCUSD\", \"ETHUSD\", \"SOLUSD\"]\n",
    "DEFAULT_RESOLUTION = Resolution.Hour\n",
    "SYMBOLS = add_crypto_universe(UNIVERSE, DEFAULT_RESOLUTION)\n",
    "\n",
    "REQUEST_START = datetime(2024, 1, 1)\n",
    "REQUEST_END = datetime.utcnow()\n",
    "\n",
    "loader = DataLoader()\n",
    "for symbol in SYMBOLS:\n",
    "    loader.register(\n",
    "        DataRequestSpec(\n",
    "            symbol=symbol.Value,\n",
    "            market=symbol.ID.Market,\n",
    "            security_type=\"Crypto\",\n",
    "            resolution=enum_to_str(DEFAULT_RESOLUTION),\n",
    "            start=REQUEST_START.isoformat(),\n",
    "            end=REQUEST_END.isoformat(),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CACHE_ROOT = Path(\"data\")\n",
    "ONCHAIN_CACHE = DATA_CACHE_ROOT / \"onchain\"\n",
    "FUNDING_CACHE = DATA_CACHE_ROOT / \"funding\"\n",
    "SENTIMENT_CACHE = DATA_CACHE_ROOT / \"sentiment\"\n",
    "DEFI_CACHE = DATA_CACHE_ROOT / \"defi\"\n",
    "TOKENOMICS_CACHE = DATA_CACHE_ROOT / \"tokenomics\"\n",
    "EXECUTION_CACHE = DATA_CACHE_ROOT / \"execution\"\n",
    "\n",
    "\n",
    "def load_cached_timeseries(file_path: Path, index_col: str = \"timestamp\") -> pd.DataFrame | None:\n",
    "    \"\"\"Load a cached time-series dataset if present; return None when absent.\"\"\"\n",
    "\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "\n",
    "    if file_path.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(file_path)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "    if index_col in df.columns:\n",
    "        df[index_col] = pd.to_datetime(df[index_col])\n",
    "        df = df.set_index(index_col)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DATA_PIPELINES = False\n",
    "PIPELINE_SYMBOLS = [symbol.Value if hasattr(symbol, \"Value\") else str(symbol) for symbol in SYMBOLS]\n",
    "PIPELINE_SYMBOLS = [sym.upper() for sym in PIPELINE_SYMBOLS]\n",
    "RUN_ONCHAIN_PIPELINE = False\n",
    "RUN_FUNDING_PIPELINE = False\n",
    "RUN_SENTIMENT_PIPELINE = False\n",
    "RUN_DEFI_PIPELINE = False\n",
    "DEFI_PROTOCOL_MAP: dict[str, str] = {}\n",
    "RUN_TOKENOMICS_PIPELINE = False\n",
    "RUN_EXECUTION_PIPELINE = False\n",
    "OVERWRITE_PIPELINE_DATA = False\n",
    "\n",
    "if RUN_DATA_PIPELINES:\n",
    "    from research.scripts.data_fetchers import (\n",
    "        onchain as onchain_fetcher,\n",
    "        funding as funding_fetcher,\n",
    "        sentiment as sentiment_fetcher,\n",
    "        defi as defi_fetcher,\n",
    "        tokenomics as tokenomics_fetcher,\n",
    "        execution as execution_fetcher,\n",
    "    )\n",
    "\n",
    "    pipeline_reports = {}\n",
    "\n",
    "    if RUN_ONCHAIN_PIPELINE:\n",
    "        pipeline_reports[\"onchain\"] = onchain_fetcher.run_pipeline(\n",
    "            PIPELINE_SYMBOLS,\n",
    "            ONCHAIN_CACHE,\n",
    "            overwrite=OVERWRITE_PIPELINE_DATA,\n",
    "        )\n",
    "\n",
    "    if RUN_FUNDING_PIPELINE:\n",
    "        pipeline_reports[\"funding\"] = funding_fetcher.run_pipeline(\n",
    "            PIPELINE_SYMBOLS,\n",
    "            FUNDING_CACHE,\n",
    "            overwrite=OVERWRITE_PIPELINE_DATA,\n",
    "        )\n",
    "\n",
    "    if RUN_SENTIMENT_PIPELINE:\n",
    "        pipeline_reports[\"sentiment\"] = sentiment_fetcher.run_pipeline(\n",
    "            PIPELINE_SYMBOLS,\n",
    "            SENTIMENT_CACHE,\n",
    "            overwrite=OVERWRITE_PIPELINE_DATA,\n",
    "        )\n",
    "\n",
    "    if RUN_DEFI_PIPELINE:\n",
    "        protocol_map = {symbol: DEFI_PROTOCOL_MAP.get(symbol, symbol.lower()) for symbol in PIPELINE_SYMBOLS}\n",
    "        pipeline_reports[\"defi\"] = defi_fetcher.run_pipeline(\n",
    "            PIPELINE_SYMBOLS,\n",
    "            protocol_map=protocol_map,\n",
    "            out_dir=DEFI_CACHE,\n",
    "            overwrite=OVERWRITE_PIPELINE_DATA,\n",
    "        )\n",
    "\n",
    "    if RUN_TOKENOMICS_PIPELINE:\n",
    "        pipeline_reports[\"tokenomics\"] = tokenomics_fetcher.run_pipeline(\n",
    "            PIPELINE_SYMBOLS,\n",
    "            TOKENOMICS_CACHE,\n",
    "            overwrite=OVERWRITE_PIPELINE_DATA,\n",
    "        )\n",
    "\n",
    "    if RUN_EXECUTION_PIPELINE:\n",
    "        pipeline_reports[\"execution\"] = execution_fetcher.run_pipeline(\n",
    "            overwrite=OVERWRITE_PIPELINE_DATA,\n",
    "        )\n",
    "\n",
    "    if pipeline_reports:\n",
    "        summary = {key: list(value.values()) if isinstance(value, dict) else value for key, value in pipeline_reports.items()}\n",
    "        if display is not None:\n",
    "            display(summary)\n",
    "        else:\n",
    "            print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Coverage Audit\n",
    "\n",
    "### Core Market Data\n",
    "- **Spot OHLCV (minute/second/tick)** — *QuantConnect native* via standard `History`/live subscriptions across crypto, FX, equities, futures.\n",
    "- **Order book depth (L2/L3)** — *External*; source from exchange websockets or vendors (Kaiko, CoinAPI), stream into Lean with custom data classes.\n",
    "- **Corporate actions (equities)** — *QuantConnect native* through Security Master; keep for cross-asset filters.\n",
    "- **Crypto exchange coverage** — *QuantConnect native* (Binance/US, Bitfinex, Bybit, Coinbase, Kraken). Extend with custom connectors if needed.\n",
    "\n",
    "### Liquidity & Trading Frictions\n",
    "- **Spreads & realized spreads, slippage proxies** — *Derived* from QC quote ticks; implement Kyle lambda/kappa offline.\n",
    "- **Fee schedules, withdrawal limits, maintenance windows** — *External*; maintain YAML/DB updated from exchange APIs.\n",
    "\n",
    "### Momentum / Volatility / Microstructure\n",
    "- **Price momentum, trend filters, breakout stats** — *QuantConnect native* indicator library (Momentum, EMA, ATR, etc.).\n",
    "- **Volume analytics (RVOL, seasonal baselines)** — *Derived* from QC volume; store transforms in feature registry.\n",
    "- **Volatility estimators (ATR, GK, realized/IV)** — *QuantConnect native* for ATR & equity IV; crypto options IV needs *external* feeds (Deribit, Amberdata).\n",
    "- **Microstructure features (trade-sign, book imbalance, reversion)** — *External* tick/L2 ingestion required.\n",
    "\n",
    "### Derivatives & Leverage (Crypto)\n",
    "- **Perp funding, open interest, basis, liquidations** — *External* via exchange REST (Binance/Bybit) or aggregators (Coinalyze, CoinGlass).\n",
    "- **Options surface (IV level/skew, term structure)** — *QuantConnect native* for equity options; crypto options require *external* sources (Deribit, GreeksLive).\n",
    "\n",
    "### On-chain Metrics\n",
    "- **Network activity (addresses, fees, hash rate, staking)** — *External* (Glassnode, CoinMetrics, Dune, Nansen).\n",
    "- **Supply & cohorts (MVRV, SOPR, age bands)** — *External* via same providers; schedule ETL to preserved Parquet.\n",
    "- **Whale activity & exchange flows** — *External* (Santiment, Whale Alert, CryptoQuant).\n",
    "- **Tokenomics calendars (unlock, burns, governance)** — *External* (TokenUnlocks, Messari).\n",
    "\n",
    "### Social / News / Sentiment\n",
    "- **News headlines + NLP** — *QuantConnect native* (Benzinga, Tiingo); limited altcoin depth.\n",
    "- **Social signals (Twitter, Reddit, Telegram)** — *External* (Santiment, LunarCrush, custom firehoses).\n",
    "- **Search trends (Google Trends)** — *External* via PyTrends or vendor.\n",
    "- **Developer activity (GitHub metrics)** — *External* (Token Terminal, GitHub API).\n",
    "- **Alt sentiment (Brain metrics)** — *QuantConnect native* for equities; use for regime context.\n",
    "\n",
    "### Fundamental / Macro\n",
    "- **Crypto market-cap ranks & dominance** — *QuantConnect native* CoinGecko dataset.\n",
    "- **Macro releases (rates, inflation, liquidity)** — *QuantConnect native* (EODHD econ/macro feeds) plus optional external calendars.\n",
    "- **Flows & positioning (ETF creations, stablecoin issuance)** — *External* (Farside, issuer APIs).\n",
    "- **Sector/theme tags (AI, DeFi, RWA, memecoins)** — *External* taxonomy + NLP classification.\n",
    "\n",
    "### Universe & Eligibility\n",
    "- **Tradability filters (turnover, price bounds, venue lists)** — *Derived* in Lean universe selection.\n",
    "- **Market-cap tiers** — *QuantConnect native* (CoinGecko) + custom rules.\n",
    "- **Symbol mapping / survivorship handling** — equities handled natively; crypto mapping is *external* (maintain cross-venue IDs).\n",
    "\n",
    "### Risk Model & Regimes\n",
    "- **Correlations, betas, clustering** — *Derived* from price history (HRP/graph clustering offline).\n",
    "- **Factor proxies (momentum, carry, size, quality, value)** — momentum/carry from QC data; quality/value require on-chain inputs -> *external*.\n",
    "- **Regime labels (trend/liquidity/volatility states)** — *Derived* using price & macro feeds.\n",
    "\n",
    "### Calendars & Events\n",
    "- **Protocol roadmaps, forks, upgrades** — *External* (project blogs, Messari).\n",
    "- **Listings/delistings, maintenance** — *External* (exchange status endpoints).\n",
    "- **Token unlocks/airdrops** — *External* (TokenUnlocks, VestLab).\n",
    "- **Legal/regulatory events** — *External* (NLP on filings/news).\n",
    "\n",
    "### Execution Support\n",
    "- **Venue selection metrics (spreads, rejects, latency)** — *Derived* from live trading telemetry; store in analytics DB.\n",
    "- **Borrow & funding availability** — *External* (prime brokers, exchange APIs).\n",
    "\n",
    "### Additional Enhancements\n",
    "- **DeFi & yield metrics (TVL, APY, peg deviation)** — *External* (DefiLlama, Messari) for rotation & stress detection.\n",
    "- **Cross-chain flows (bridge volumes, wrapped supply)** — *External* (Wormhole, LayerZero stats, Nansen).\n",
    "- **Custody/counterparty risk (PoR, sanctions)** — *External* monitoring feeds.\n",
    "- **Security alerts (smart-contract exploits)** — *External* (CertiK, BlockSec) feeding risk breakers.\n",
    "- **Futures curve aggregation (CME vs offshore)** — *Hybrid*; CME via QC futures, offshore basis external.\n",
    "- **Stablecoin internals (issuer reserves, velocity)** — *External* issuer reports + on-chain data.\n",
    "- **Alternative engagement (Discord/YouTube)** — *External* scrapers + NLP.\n",
    "- **TCA/latency footprints** — *Derived* nightly analyses of execution logs.\n",
    "\n",
    "### Integration Game Plan\n",
    "1. Tag every feed with `DataRequestSpec` metadata (source, cadence, licensing) for reproducibility.\n",
    "2. For external datasets, implement Lean custom data classes (`BaseData`) and schedule ETL (REST, websockets, or offline batch).\n",
    "3. Store normalized outputs (Parquet/S3) and promote reusable loaders into `research/scripts`.\n",
    "4. Validate new datasets in the `selection_booming_assets` and `selection_risk_controls` notebooks before rolling into live algos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Snapshots\n",
    "\n",
    "Flip the switches below to grab and visualize recent history for any subset of `SYMBOLS`. Each block focuses on a different feature family so we can sanity-check distributions before wiring them into selection signals. Additional sections cover volatility/drawdowns and cross-asset structure so we can iterate quickly once new datasets land.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_PRICE_VOLUME = True\n",
    "PLOT_LOOKBACK = timedelta(days=30)\n",
    "PLOT_SYMBOLS = SYMBOLS\n",
    "\n",
    "if VISUALIZE_PRICE_VOLUME:\n",
    "    history = qb.History(PLOT_SYMBOLS, PLOT_LOOKBACK, DEFAULT_RESOLUTION)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"History request returned empty frame. Check data permissions or timeframe.\")\n",
    "\n",
    "    closes = history.close.unstack(level=0).tz_localize(None)\n",
    "    volumes = history.volume.unstack(level=0).tz_localize(None)\n",
    "\n",
    "    fig, axes = plt.subplots(len(PLOT_SYMBOLS), 1, figsize=(16, 4.8 * len(PLOT_SYMBOLS)), sharex=True)\n",
    "    if len(PLOT_SYMBOLS) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, symbol in zip(axes, PLOT_SYMBOLS):\n",
    "        symbol_key = symbol.Value\n",
    "        price_series = closes[symbol_key]\n",
    "        volume_series = volumes[symbol_key]\n",
    "\n",
    "        ax.plot(price_series.index, price_series, color=\"steelblue\", linewidth=1.3, label=\"Close\")\n",
    "        ax.set_ylabel(\"Price\")\n",
    "        ax.set_title(f\"{symbol_key} Price & Volume\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.tick_params(axis=\"both\", labelsize=9)\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        if len(volume_series) > 1:\n",
    "            bar_width = (mdates.date2num(volume_series.index[1]) - mdates.date2num(volume_series.index[0])) * 0.75\n",
    "        else:\n",
    "            bar_width = 0.03\n",
    "        ax2.bar(volume_series.index, volume_series, width=bar_width, color=\"gray\", alpha=0.25, label=\"Volume\")\n",
    "        ax2.set_ylabel(\"Volume\")\n",
    "        ax2.legend(loc=\"upper right\")\n",
    "        ax2.tick_params(axis=\"both\", labelsize=9)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_MOMENTUM_FACTORS = True\n",
    "MOM_LOOKBACK = timedelta(days=120)\n",
    "MOM_WINDOWS = (24, 72, 168)  # hours\n",
    "RVOL_WINDOW = 72  # hours\n",
    "\n",
    "if VISUALIZE_MOMENTUM_FACTORS:\n",
    "    history = qb.History(PLOT_SYMBOLS, MOM_LOOKBACK, DEFAULT_RESOLUTION)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"History request returned empty frame. Check data permissions or timeframe.\")\n",
    "\n",
    "    closes = history.close.unstack(level=0).tz_localize(None)\n",
    "    volumes = history.volume.unstack(level=0).tz_localize(None)\n",
    "\n",
    "    rows = len(PLOT_SYMBOLS) * 2\n",
    "    fig, axes = plt.subplots(rows, 1, figsize=(16, 6.0 * len(PLOT_SYMBOLS)), sharex=True)\n",
    "    if rows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, symbol in enumerate(PLOT_SYMBOLS):\n",
    "        price_ax = axes[2 * idx]\n",
    "        rvol_ax = axes[2 * idx + 1]\n",
    "\n",
    "        symbol_key = symbol.Value\n",
    "        price_series = closes[symbol_key]\n",
    "        price_ax.plot(price_series.index, price_series, color=\"black\", linewidth=1.3, label=\"Close\")\n",
    "\n",
    "        momentum_ax = price_ax.twinx()\n",
    "        for window in MOM_WINDOWS:\n",
    "            momentum = price_series.pct_change(window)\n",
    "            momentum_ax.plot(momentum.index, momentum, linewidth=1.1, label=f\"ROC {window}h\")\n",
    "        price_ax.set_ylabel(\"Price\")\n",
    "        price_ax.set_title(f\"{symbol_key} Price & Momentum\")\n",
    "        price_ax.legend(loc=\"upper left\", fontsize=9)\n",
    "        momentum_ax.set_ylabel(\"Return\")\n",
    "        momentum_ax.legend(loc=\"upper right\", fontsize=9)\n",
    "        price_ax.grid(alpha=0.25)\n",
    "        price_ax.tick_params(axis=\"both\", labelsize=10)\n",
    "        momentum_ax.tick_params(axis=\"both\", labelsize=10)\n",
    "\n",
    "        rvol_series = volumes[symbol_key] / volumes[symbol_key].rolling(RVOL_WINDOW).mean()\n",
    "        rvol_ax.plot(rvol_series.index, rvol_series, color=\"tab:orange\", linewidth=1.2)\n",
    "        rvol_ax.axhline(1.0, color=\"gray\", linestyle=\"--\", linewidth=1.0)\n",
    "        rvol_ax.set_ylabel(\"rVOL (×)\")\n",
    "        rvol_ax.set_title(f\"{symbol_key} Relative Volume\")\n",
    "        rvol_ax.grid(alpha=0.25)\n",
    "        rvol_ax.tick_params(axis=\"both\", labelsize=10)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_RETURN_DISTRIBUTIONS = True\n",
    "RET_LOOKBACK = timedelta(days=90)\n",
    "RET_RESOLUTION = Resolution.Hour\n",
    "\n",
    "if VISUALIZE_RETURN_DISTRIBUTIONS:\n",
    "    history = qb.History(PLOT_SYMBOLS, RET_LOOKBACK, RET_RESOLUTION)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"History request returned empty frame. Check data permissions or timeframe.\")\n",
    "\n",
    "    closes = history.close.unstack(level=0).tz_localize(None)\n",
    "    returns = closes.pct_change().dropna()\n",
    "\n",
    "    fig, axes = plt.subplots(len(PLOT_SYMBOLS), 1, figsize=(12, 4.5 * len(PLOT_SYMBOLS)))\n",
    "    if len(PLOT_SYMBOLS) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    resolution_label = enum_to_str(RET_RESOLUTION)\n",
    "\n",
    "    for ax, symbol in zip(axes, PLOT_SYMBOLS):\n",
    "        symbol_key = symbol.Value\n",
    "        data = returns[symbol_key].dropna()\n",
    "        ax.hist(data, bins=50, alpha=0.75, color=\"mediumpurple\", edgecolor=\"black\", linewidth=0.4)\n",
    "        ax.axvline(data.mean(), color=\"red\", linestyle=\"--\", linewidth=1, label=\"Mean\")\n",
    "        ax.set_title(f\"{symbol_key} {resolution_label} Return Distribution\")\n",
    "        ax.set_xlabel(\"Return\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.tick_params(axis=\"both\", labelsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility & Drawdown Diagnostics\n",
    "Use these panels to gauge risk signature: rolling realized volatility (multiple horizons), ATR as a percent of price, and max drawdown depth. Helpful for spotting regime shifts and sizing constraints before ranking assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_VOL_RISK = True\n",
    "VOL_LOOKBACK = timedelta(days=180)\n",
    "REALIZED_WINDOWS = (24, 168)\n",
    "ATR_WINDOW = 24\n",
    "\n",
    "if VISUALIZE_VOL_RISK:\n",
    "    history = qb.History(PLOT_SYMBOLS, VOL_LOOKBACK, DEFAULT_RESOLUTION)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"History request returned empty frame. Check data permissions or timeframe.\")\n",
    "\n",
    "    closes = history.close.unstack(level=0).tz_localize(None)\n",
    "    highs = history.high.unstack(level=0).tz_localize(None)\n",
    "    lows = history.low.unstack(level=0).tz_localize(None)\n",
    "    returns = closes.pct_change().dropna()\n",
    "\n",
    "    fig, axes = plt.subplots(len(PLOT_SYMBOLS), 3, figsize=(20, 5.2 * len(PLOT_SYMBOLS)), sharex='col')\n",
    "    if len(PLOT_SYMBOLS) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for row_axes, symbol in zip(axes, PLOT_SYMBOLS):\n",
    "        price_ax, vol_ax, dd_ax = row_axes\n",
    "        symbol_key = symbol.Value\n",
    "        price_series = closes[symbol_key].dropna()\n",
    "        high_series = highs[symbol_key].dropna()\n",
    "        low_series = lows[symbol_key].dropna()\n",
    "        ret_series = returns[symbol_key].dropna()\n",
    "\n",
    "        price_ax.plot(price_series.index, price_series, color='steelblue', linewidth=1.3)\n",
    "        price_ax.set_title(f\"{symbol_key} Price\")\n",
    "        price_ax.set_ylabel(\"Price\")\n",
    "        price_ax.grid(alpha=0.25)\n",
    "        price_ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "        for window in REALIZED_WINDOWS:\n",
    "            realized = ret_series.rolling(window).std() * np.sqrt(window)\n",
    "            vol_ax.plot(realized.index, realized, linewidth=1.2, label=f\"σ_{window}h\")\n",
    "        true_range = pd.concat([\n",
    "            (high_series - low_series),\n",
    "            (high_series - price_series.shift(1)).abs(),\n",
    "            (low_series - price_series.shift(1)).abs()\n",
    "        ], axis=1).max(axis=1)\n",
    "        atr = true_range.rolling(ATR_WINDOW).mean()\n",
    "        atr_pct = (atr / price_series).rename('ATR%')\n",
    "        vol_ax.plot(atr_pct.index, atr_pct, color='tab:orange', linewidth=1.1, label='ATR%')\n",
    "        vol_ax.set_title(f\"{symbol_key} Realized Vol & ATR%\")\n",
    "        vol_ax.set_ylabel(\"Volatility\")\n",
    "        vol_ax.legend(loc='upper right', fontsize=8)\n",
    "        vol_ax.grid(alpha=0.25)\n",
    "        vol_ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "        rolling_max = price_series.cummax()\n",
    "        drawdown = price_series / rolling_max - 1\n",
    "        dd_ax.fill_between(drawdown.index, drawdown, 0, color='tomato', alpha=0.4)\n",
    "        dd_ax.plot(drawdown.index, drawdown, color='tomato', linewidth=1.0)\n",
    "        dd_ax.set_title(f\"{symbol_key} Drawdown\")\n",
    "        dd_ax.set_ylabel(\"Drawdown\")\n",
    "        dd_ax.grid(alpha=0.25)\n",
    "        dd_ax.tick_params(axis='both', labelsize=10)\n",
    "        dd_ax.set_ylim(min(drawdown.min() * 1.1, -0.05), 0.02)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Asset Structure\n",
    "Correlation and co-movement diagnostics help us understand diversification and shared drivers across the candidate universe. These panels will grow as we add macro and sentiment overlays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Chain Activity\n",
    "Visual inspection of on-chain flow helps validate whether capital is entering or leaving venues. Plug in Glassnode/CoinMetrics/Nansen extracts (saved under `data/onchain/`) to light up active addresses, exchange netflows, and whale transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_ONCHAIN = False\n",
    "ONCHAIN_FIELDS = [\n",
    "    (\"active_addresses\", \"Active Addresses\"),\n",
    "    (\"exchange_netflow\", \"Exchange Netflow\"),\n",
    "    (\"whale_tx_usd\", \"Whale Tx Volume (USD)\")\n",
    "]\n",
    "\n",
    "if VISUALIZE_ONCHAIN:\n",
    "    available = []\n",
    "    missing = []\n",
    "    for symbol in PLOT_SYMBOLS:\n",
    "        symbol_key = symbol.Value\n",
    "        cache_file = ONCHAIN_CACHE / f\"{symbol_key.lower()}_onchain.parquet\"\n",
    "        df = load_cached_timeseries(cache_file)\n",
    "        if df is None or df.empty:\n",
    "            missing.append(symbol_key)\n",
    "            continue\n",
    "        available.append((symbol_key, df))\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing on-chain cache for: {', '.join(missing)}\")\n",
    "\n",
    "    if not available:\n",
    "        raise FileNotFoundError(\n",
    "            \"No on-chain cache located. Populate data/onchain/<symbol>_onchain.parquet via your ETL pipeline.\"\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(len(available), len(ONCHAIN_FIELDS), figsize=(20, 5 * len(available)), sharex='col')\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for row_axes, (symbol_key, df) in zip(axes, available):\n",
    "        for ax, (field, title) in zip(row_axes, ONCHAIN_FIELDS):\n",
    "            if field not in df.columns:\n",
    "                ax.text(0.5, 0.5, f\"Field '{field}' not found\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            series = df[field].dropna()\n",
    "            if series.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            ax.plot(series.index, series, linewidth=1.2, color='teal')\n",
    "            ax.set_title(f\"{symbol_key} {title}\")\n",
    "            ax.grid(alpha=0.25)\n",
    "            ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funding & Derivatives\n",
    "Hook in Coinalyze/CoinGlass/Deribit extracts under `data/funding/` to monitor perp funding regimes, open-interest builds, and spot-perp basis moves before we commit capital.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_FUNDING = False\n",
    "FUNDING_FIELDS = [\n",
    "    (\"funding_rate\", \"Funding Rate (%)\"),\n",
    "    (\"open_interest_usd\", \"Open Interest (USD)\"),\n",
    "    (\"basis_pct\", \"Spot-Perp Basis (%)\")\n",
    "]\n",
    "\n",
    "if VISUALIZE_FUNDING:\n",
    "    available = []\n",
    "    missing = []\n",
    "    for symbol in PLOT_SYMBOLS:\n",
    "        symbol_key = symbol.Value\n",
    "        cache_file = FUNDING_CACHE / f\"{symbol_key.lower()}_funding.parquet\"\n",
    "        df = load_cached_timeseries(cache_file)\n",
    "        if df is None or df.empty:\n",
    "            missing.append(symbol_key)\n",
    "            continue\n",
    "        available.append((symbol_key, df))\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing funding cache for: {', '.join(missing)}\")\n",
    "\n",
    "    if not available:\n",
    "        raise FileNotFoundError(\n",
    "            \"No funding cache located. Populate data/funding/<symbol>_funding.parquet via your ETL pipeline.\"\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(len(available), len(FUNDING_FIELDS), figsize=(20, 5 * len(available)), sharex='col')\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for row_axes, (symbol_key, df) in zip(axes, available):\n",
    "        for ax, (field, title) in zip(row_axes, FUNDING_FIELDS):\n",
    "            if field not in df.columns:\n",
    "                ax.text(0.5, 0.5, f\"Field '{field}' not found\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            series = df[field].dropna()\n",
    "            if series.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            ax.plot(series.index, series, linewidth=1.2, color='slateblue')\n",
    "            ax.set_title(f\"{symbol_key} {title}\")\n",
    "            ax.grid(alpha=0.25)\n",
    "            ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment & Attention\n",
    "Drop Santiment/LunarCrush/Twitter aggregates into `data/sentiment/` to inspect social dominance, polarity, and mention velocity alongside price action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_SENTIMENT = False\n",
    "SENTIMENT_FIELDS = [\n",
    "    (\"social_dominance\", \"Social Dominance (%)\"),\n",
    "    (\"sentiment_score\", \"Sentiment Score\"),\n",
    "    (\"mention_velocity\", \"Mention Velocity\")\n",
    "]\n",
    "\n",
    "if VISUALIZE_SENTIMENT:\n",
    "    available = []\n",
    "    missing = []\n",
    "    for symbol in PLOT_SYMBOLS:\n",
    "        symbol_key = symbol.Value\n",
    "        cache_file = SENTIMENT_CACHE / f\"{symbol_key.lower()}_sentiment.parquet\"\n",
    "        df = load_cached_timeseries(cache_file)\n",
    "        if df is None or df.empty:\n",
    "            missing.append(symbol_key)\n",
    "            continue\n",
    "        available.append((symbol_key, df))\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing sentiment cache for: {', '.join(missing)}\")\n",
    "\n",
    "    if not available:\n",
    "        raise FileNotFoundError(\n",
    "            \"No sentiment cache located. Populate data/sentiment/<symbol>_sentiment.parquet via your ETL pipeline.\"\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(len(available), len(SENTIMENT_FIELDS), figsize=(20, 5 * len(available)), sharex='col')\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for row_axes, (symbol_key, df) in zip(axes, available):\n",
    "        for ax, (field, title) in zip(row_axes, SENTIMENT_FIELDS):\n",
    "            if field not in df.columns:\n",
    "                ax.text(0.5, 0.5, f\"Field '{field}' not found\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            series = df[field].dropna()\n",
    "            if series.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            ax.plot(series.index, series, linewidth=1.2, color='seagreen')\n",
    "            ax.set_title(f\"{symbol_key} {title}\")\n",
    "            ax.grid(alpha=0.25)\n",
    "            ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeFi & Liquidity\n",
    "Cached DefiLlama/Messari pulls under `data/defi/` (pool TVL, borrow/supply rates, liquidity fragmentation) make it easy to see whether on-chain liquidity is following our breakout candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_DEFI = False\n",
    "DEFI_FIELDS = [\n",
    "    (\"tvl_usd\", \"TVL (USD)\"),\n",
    "    (\"borrow_rate\", \"Borrow Rate (%)\"),\n",
    "    (\"supply_rate\", \"Supply Rate (%)\")\n",
    "]\n",
    "\n",
    "if VISUALIZE_DEFI:\n",
    "    available = []\n",
    "    missing = []\n",
    "    for symbol in PLOT_SYMBOLS:\n",
    "        symbol_key = symbol.Value\n",
    "        cache_file = DEFI_CACHE / f\"{symbol_key.lower()}_defi.parquet\"\n",
    "        df = load_cached_timeseries(cache_file)\n",
    "        if df is None or df.empty:\n",
    "            missing.append(symbol_key)\n",
    "            continue\n",
    "        available.append((symbol_key, df))\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing DeFi cache for: {', '.join(missing)}\")\n",
    "\n",
    "    if not available:\n",
    "        raise FileNotFoundError(\n",
    "            \"No DeFi cache located. Populate data/defi/<symbol>_defi.parquet via your ETL pipeline.\"\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(len(available), len(DEFI_FIELDS), figsize=(20, 5 * len(available)), sharex='col')\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for row_axes, (symbol_key, df) in zip(axes, available):\n",
    "        for ax, (field, title) in zip(row_axes, DEFI_FIELDS):\n",
    "            if field not in df.columns:\n",
    "                ax.text(0.5, 0.5, f\"Field '{field}' not found\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            series = df[field].dropna()\n",
    "            if series.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', fontsize=10)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "            ax.plot(series.index, series, linewidth=1.2, color='darkorange')\n",
    "            ax.set_title(f\"{symbol_key} {title}\")\n",
    "            ax.grid(alpha=0.25)\n",
    "            ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenomics & Events\n",
    "Store unlock/burn/governance schedules under `data/tokenomics/` to surface upcoming supply shocks that may impact the selection list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_TOKENOMICS = False\n",
    "TOKEN_EVENT_FILES = (\n",
    "    \"_events.parquet\",\n",
    "    \"_events.csv\"\n",
    ")\n",
    "\n",
    "if VISUALIZE_TOKENOMICS:\n",
    "    missing = []\n",
    "    for symbol in PLOT_SYMBOLS:\n",
    "        symbol_key = symbol.Value\n",
    "        dataset = None\n",
    "        for suffix in TOKEN_EVENT_FILES:\n",
    "            candidate = TOKENOMICS_CACHE / f\"{symbol_key.lower()}{suffix}\"\n",
    "            if candidate.exists():\n",
    "                if candidate.suffix == \".parquet\":\n",
    "                    dataset = pd.read_parquet(candidate)\n",
    "                else:\n",
    "                    dataset = pd.read_csv(candidate)\n",
    "                break\n",
    "        if dataset is None or dataset.empty:\n",
    "            missing.append(symbol_key)\n",
    "            continue\n",
    "\n",
    "        date_cols = [\n",
    "            col for col in (\"event_time\", \"start_time\", \"end_time\", \"start\", \"end\")\n",
    "            if col in dataset.columns\n",
    "        ]\n",
    "        for col in date_cols:\n",
    "            dataset[col] = pd.to_datetime(dataset[col])\n",
    "        dataset = dataset.sort_values(date_cols[0] if date_cols else dataset.columns[0])\n",
    "\n",
    "        print(f\"Tokenomics timeline for {symbol_key}\")\n",
    "        columns_to_show = [col for col in dataset.columns if col not in {\"details\", \"notes\"}]\n",
    "        preview = dataset[columns_to_show].head(10)\n",
    "        if display is not None:\n",
    "            display(preview)\n",
    "        else:\n",
    "            print(preview.to_string(index=False))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    if missing and len(missing) == len(PLOT_SYMBOLS):\n",
    "        raise FileNotFoundError(\n",
    "            \"No tokenomics schedules found. Populate data/tokenomics/<symbol>_events.(csv|parquet).\"\n",
    "        )\n",
    "    elif missing:\n",
    "        print(f\"Missing tokenomics schedules for: {', '.join(missing)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Analytics\n",
    "After running paper/live algos, export venue fill logs under `data/execution/` so we can review slippage, latency, and fill efficiency before promoting selection ideas into production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_EXECUTION = False\n",
    "EXECUTION_METRICS_FILE = EXECUTION_CACHE / \"execution_metrics.parquet\"\n",
    "\n",
    "if VISUALIZE_EXECUTION:\n",
    "    metrics = load_cached_timeseries(EXECUTION_METRICS_FILE, index_col=\"timestamp\")\n",
    "    if metrics is None or metrics.empty:\n",
    "        raise FileNotFoundError(\n",
    "            \"No execution metrics found. Populate data/execution/execution_metrics.parquet with fill logs.\"\n",
    "        )\n",
    "\n",
    "    if \"symbol\" not in metrics.columns:\n",
    "        raise ValueError(\"Execution metrics file must include a 'symbol' column.\")\n",
    "\n",
    "    metrics.index = pd.to_datetime(metrics.index)\n",
    "    metrics = metrics.sort_index()\n",
    "\n",
    "    groups = list(metrics.groupby(\"symbol\"))\n",
    "    fig, axes = plt.subplots(len(groups), 2, figsize=(16, 5 * len(groups)), sharex=False)\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for (symbol_key, df_symbol), row_axes in zip(groups, axes):\n",
    "        slippage_ax, latency_ax = row_axes\n",
    "        df_symbol = df_symbol.sort_index()\n",
    "\n",
    "        if \"slippage_bps\" in df_symbol.columns:\n",
    "            slippage_series = df_symbol[\"slippage_bps\"].astype(float)\n",
    "            slippage_ax.plot(slippage_series.index, slippage_series, color=\"indigo\", alpha=0.7, linewidth=1.1)\n",
    "            slippage_ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "            slippage_ax.set_ylabel(\"Slippage (bps)\")\n",
    "            slippage_ax.grid(alpha=0.25)\n",
    "            slippage_ax.tick_params(axis=\"both\", labelsize=9)\n",
    "        else:\n",
    "            slippage_ax.text(0.5, 0.5, \"'slippage_bps' column missing\", ha='center', va='center', fontsize=10)\n",
    "            slippage_ax.set_axis_off()\n",
    "\n",
    "        slippage_ax.set_title(f\"{symbol_key} Slippage\")\n",
    "\n",
    "        if \"latency_ms\" in df_symbol.columns:\n",
    "            latency_series = df_symbol[\"latency_ms\"].dropna().astype(float)\n",
    "            latency_ax.hist(latency_series, bins=40, color=\"darkcyan\", alpha=0.75, edgecolor=\"black\", linewidth=0.3)\n",
    "            latency_ax.set_xlabel(\"Latency (ms)\")\n",
    "            latency_ax.set_ylabel(\"Frequency\")\n",
    "            latency_ax.grid(alpha=0.25)\n",
    "            latency_ax.tick_params(axis=\"both\", labelsize=9)\n",
    "        else:\n",
    "            latency_ax.text(0.5, 0.5, \"'latency_ms' column missing\", ha='center', va='center', fontsize=10)\n",
    "            latency_ax.set_axis_off()\n",
    "\n",
    "        if \"fill_rate\" in df_symbol.columns:\n",
    "            fill_ax = latency_ax.twinx()\n",
    "            fill_series = df_symbol[\"fill_rate\"].rolling(window=50, min_periods=10).mean()\n",
    "            fill_ax.plot(df_symbol.index, fill_series, color=\"tomato\", linewidth=1.0, label=\"Rolling Fill Rate\")\n",
    "            fill_ax.set_ylabel(\"Fill Rate\")\n",
    "            fill_ax.legend(loc=\"upper right\", fontsize=8)\n",
    "            fill_ax.tick_params(axis=\"y\", labelsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Analytics\n",
    "After running paper/live algos, export venue fill logs under `data/execution/` so we can review slippage, latency, and fill efficiency before promoting selection ideas into production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_CROSS_SECTION = True\n",
    "CORR_LOOKBACK = timedelta(days=90)\n",
    "\n",
    "if VISUALIZE_CROSS_SECTION:\n",
    "    history = qb.History(PLOT_SYMBOLS, CORR_LOOKBACK, DEFAULT_RESOLUTION)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"History request returned empty frame. Check data permissions or timeframe.\")\n",
    "\n",
    "    closes = history.close.unstack(level=0).tz_localize(None)\n",
    "    returns = closes.pct_change().dropna()\n",
    "\n",
    "    corr_matrix = returns.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6 + len(PLOT_SYMBOLS) * 0.6, 5 + len(PLOT_SYMBOLS) * 0.3))\n",
    "    im = ax.imshow(corr_matrix, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "    ax.set_xticks(range(len(corr_matrix.columns)))\n",
    "    ax.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(corr_matrix.index)))\n",
    "    ax.set_yticklabels(corr_matrix.index)\n",
    "    ax.set_title('Hourly Return Correlation Heatmap')\n",
    "    for i in range(len(corr_matrix.index)):\n",
    "        for j in range(len(corr_matrix.columns)):\n",
    "            ax.text(j, i, f\"{corr_matrix.iloc[i, j]:.2f}\", ha='center', va='center', color='black', fontsize=9)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if len(PLOT_SYMBOLS) > 1:\n",
    "        base_symbol = PLOT_SYMBOLS[0]\n",
    "        base_key = base_symbol.Value\n",
    "        base_returns = returns[base_key]\n",
    "        other_symbols = PLOT_SYMBOLS[1:]\n",
    "\n",
    "        fig, axes = plt.subplots(len(other_symbols), 1, figsize=(8, 4.5 * len(other_symbols)), sharex=True)\n",
    "        if len(other_symbols) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, symbol in zip(axes, other_symbols):\n",
    "            symbol_key = symbol.Value\n",
    "            data = returns[[base_key, symbol_key]].dropna()\n",
    "            ax.scatter(data[base_key], data[symbol_key], alpha=0.4, s=18, color='steelblue')\n",
    "            coeff = np.polyfit(data[base_key], data[symbol_key], 1)\n",
    "            fit_line = np.poly1d(coeff)\n",
    "            xs = np.linspace(data[base_key].min(), data[base_key].max(), 100)\n",
    "            ax.plot(xs, fit_line(xs), color='tomato', linewidth=1.2)\n",
    "            ax.set_title(f\"{symbol_key} vs {base_key} Hourly Returns (ρ={data.corr().iloc[0, 1]:.2f})\")\n",
    "            ax.set_xlabel(f\"{base_key} Return\")\n",
    "            ax.set_ylabel(f\"{symbol_key} Return\")\n",
    "            ax.grid(alpha=0.25)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Read the Visuals\n",
    "- **Price & Volume**: Left axis plots close prices; right axis overlays volume bars. Look for price thrusts backed by rising volume and note venues/dates with drying liquidity.\n",
    "- **Momentum & Relative Volume**: Left panel overlays multi-horizon rate-of-change on price; right panel tracks relative volume vs. a rolling baseline. Sustained ROC above zero with rVOL > 1 signals supportive flows; sharp reversals without volume confirm fade setups.\n",
    "- **Return Distributions**: Histograms show the distribution of hourly returns with the mean marked in red. Skew/tails hint at breakout regimes vs. choppy mean-reversion environments and help calibrate risk budgets.\n",
    "- **Volatility & Drawdown Diagnostics**: Realized-vol curves vs. ATR% reveal regime transitions (expanding/tightening ranges); drawdown panels show how deep we’ve dipped relative to recent peaks for sizing and stop placement.\n",
    "- **Cross-Asset Structure**: Correlation heatmaps and scatter plots highlight clustering, contagion risk, and diversification pockets—useful when constructing baskets or throttling overlapping bets.\n",
    "- **On-Chain Activity**: Active addresses, netflows, and whale transactions confirm whether blockchain participation supports price moves or signals rotation risk.\n",
    "- **Funding & Derivatives**: Funding-rate flips, OI builds, and basis trends expose leverage stress points and crowded trades that can accelerate breakouts or liquidations.\n",
    "- **Sentiment & Attention**: Social dominance, polarity, and mention velocity indicate whether the crowd is amplifying or fading the move; divergence vs. price often marks exhaustion.\n",
    "- **DeFi & Liquidity**: TVL migration and on-chain rates reveal where liquidity is rotating; collapsing TVL or rising borrow costs can undercut otherwise strong momentum.\n",
    "- **Tokenomics & Events**: Upcoming unlocks/burns/governance votes flag supply shocks or catalysts and help schedule entries/exits.\n",
    "- **Execution Analytics**: Slippage, latency, and fill-rate diagnostics guide venue selection, sizing, and kill-switch thresholds before deploying live capital.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SAMPLE_HISTORY = False\n",
    "\n",
    "if RUN_SAMPLE_HISTORY:\n",
    "    lookback = timedelta(days=90)\n",
    "    history = qb.History(SYMBOLS, lookback, DEFAULT_RESOLUTION)\n",
    "    price_panel = (\n",
    "        history.close.unstack(level=0)\n",
    "        .tz_localize(None)\n",
    "        .ffill()\n",
    "    )\n",
    "    price_panel.tail()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation-Autogluon",
   "language": "python",
   "name": "foundation-autogluon"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
